{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YbP8sebiKP-"
      },
      "source": [
        "# Systemy rekomendacyjne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV2nuI-iiKP_"
      },
      "source": [
        "## Wstęp\n",
        "\n",
        "Celem laboratorium jest poznanie podstaw systemów rekomendacyjnych. Zapoznasz się na nim z następującymi tematami:\n",
        "* budową systemów rekomendacyjnych typu collaborative filtering (CF), w szczególności z:\n",
        "  * macierzą interakcji użytkownik-przedmiot (user-item matrix)\n",
        "  * pojęciem biasu użytkownika i przedmiotu\n",
        "  * analizą zbiorów danych do CF\n",
        "  * metrykami jakości dla systemów rekomendacyjnych\n",
        "* algorytmami globalnej rekomendacji:\n",
        "  * metodami podstawowymi (baselines)\n",
        "  * metodami bayesowskimi (Bayesian average)\n",
        "* algorytmami personalizowanej rekomendacji typu CF, w szczególności z:\n",
        "  * najbliższych sąsiadów (neighborhood-based) typu user-based oraz item-based\n",
        "  * rozkładem macierzowym (matrix factorization) typu MF oraz FunkSVD\n",
        "\n",
        "Jak zwykle, możesz albo korzystać z Google Colab, albo z własnego komputera. W obu przypadkach trzeba doinstalować trochę bibliotek.\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/sztuczna-inteligencja/blob/master/lab7/lab_7.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrYq5CPeiKP_"
      },
      "source": [
        "## Krótki wstęp teoretyczny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejHfk72iKQA"
      },
      "source": [
        "**Systemy rekomendacyjne (recommender systems)** to dowolne metody mające rekomendować użytkownikom (users) pewne przedmioty (items). Korzysta z nich praktycznie każda większa firma: Netflix (filmy - \"Top picks for you\"), Spotify (muzyka, \"Recommended for playlist\"), Amazon (sklep - \"frequently bought together) etc. Mają niesamowicie praktyczne zastosowanie i są jednym z najwcześniej oraz najczęściej wdrażanych metod uczenia maszynowego.\n",
        "\n",
        "Jest to bardzo szeroka dziedzina, o bardzo różnorodnych podejściach. W szczególności można wyróżnić grupy:\n",
        "1. **Collaborative filtering (CF)** - oparte o historię interakcji użytkowników z przedmiotami, czyli zwykle o historię ocen. Stąd pochodzą np. rekomendacje \"użytkownicy podobni do ciebie oglądali także X\", gdzie podobieństwo mierzy się na podstawie tego, jak bardzo podobne mieliśmy w przeszłości oceny do innych użytkowników. Co ważne, takie podejście nie wymaga żadnej inżynierii cech, a jedynie zapamiętania historii ocen / transakcji / interakcji!\n",
        "2. **Content-based (CB)** - dużo bardziej podobne do klasycznego ML, tworzymy wektory cech dla przedmiotów, użytkowników i wykorzystujemy je w klasyfikacji (np. rekomendować lub nie) lub regresji (np. liczba gwiazdek).\n",
        "3. **Algorytmy hybrydowe** - łączące podejścia CF i CB podczas nauki. Są zazwyczaj bardziej złożone i wymagają odpowiednio dużych zbiorów danych.\n",
        "\n",
        "Dodatkowo możemy podzielić problemy rekomendacji na dwa rodzaje, w zależności od tego, czym są nasze **oceny (ratings)**:\n",
        "1. **Explicit feedback** - kiedy użytkownicy jawnie podają oceny, np. ocena hotelu w skali 1-10, liczba gwiadek dla przedmiotu. Wymaga to większej proaktywności użytkowników, więc potencjalnie możemy mieć mniej danych, ale są często bardziej precyzyjne. Są też typowo prostsze teoretycznie (matematycznie), bo mają znany z góry, ograniczony zakres możliwych wartości.\n",
        "2. **Implicit feedback** - kiedy jakość przedmiotu wyznaczają akcje użytkowników, np. liczba kliknięć, liczba udostępnień. Takie informacje można gromadzić automatycznie i bardzo łatwo, ale mogą być mało precyzyjne (np. przypadkowe kliknięcia, boty). Algorytmy dla takich problemów są też cięższe do zaprojektowania, bo mamy tylko wartości nieujemne i typowo nieograniczone z góry.\n",
        "\n",
        "Same rekomendacje mogą być dwojakiego rodzaju:\n",
        "1. **Globalne (global)** - biorą pod uwagę ogólne cechy przedmiotu i są oceniane dla całej społeczności, nie pod konkretnych użytkowników. Korzystają z nich typowo strony z wiadomościami, żeby ułożyć kolejność postów na stronie, np. HackerNews, Reddit. Przydają się też, gdy nie mamy dość informacji o użytkowniku, aby dokonać personalizacji.\n",
        "2. **Personalizowane (personalized)** - zasadnicze zastosowanie systemów rekomendacyjnych, w którym \"profilujemy\" użytkownika lub przedmiot, tak, aby nauczyć sie relacji między nimi i sugerować to, co konkretną osobę może interesować.\n",
        "\n",
        "Na tym laboratorium skupimy się na systemach typu collaborative filtering, bo są:\n",
        "1. Ciekawsze i bardziej unikatowe na tle tych algorytmów, które już poznaliśmy.\n",
        "2. Często o wiele łatwiejsze w praktycznej implementacji, gdyż nie wymagają feature engineeringu.\n",
        "3. Bardzo szybkie i skalowalne.\n",
        "4. Zazwyczaj lepsze pod względem wyników od systemów content-based.\n",
        "\n",
        "Skupimy się na systemach typu explicit ranking, bo są nieco prostsze i popularniejsze. Poznamy za to i systemy globalne, i personalizowane.\n",
        "\n",
        "Czemu więc korzystać z innego podejścia niż CF? O tym przekonasz się w późniejszej części labu :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubV__gtFiKQA"
      },
      "source": [
        "## Biblioteki do systemów rekomendacyjnych\n",
        "\n",
        "Do systemów rekomendacyjnych nie istnieje jedna standardowa, powszechnie przyjęta biblioteka, taka jak Scikit-learn. Jest to bowiem zbyt rozległa dziedzina, oparta o bardzo różnorodne podejścia i rozwiązania, aby dało się ją zamknąć w jednej bibliotece ze spójnym interfejsem. Można jednak wyróżnić zbiór najpopularniejszych bibliotek. Co ważne, praktyczne systemy implementuje się jednak często od zera, pod konkretny problem.\n",
        "\n",
        "1. [Surprise](https://surpriselib.com/) - od niedawna `scikit-surprise` ze względu na implementację interfejsów ze Scikit-learn'a. Implementuje algorytmy typu explicit rating collaborative filtering.\n",
        "2. [Implicit](https://benfred.github.io/implicit/) - podobna do Surprise, implementuje algorytmy typu implicit rating collaborative filtering.\n",
        "3. [LibRecommender](https://github.com/massquantity/LibRecommender) - rozbudowana biblioteka, implementująca różne podejścia: collaborative filtering, feature-based, oraz hybrydowe. Zawiera algorytmy pisane od zera, w TensorFlow (niestety v1) oraz w PyTorchu, z wielu artykułów naukowych. Ma jednak dość specyficzny, niekoniecznie intuicyjny interfejs.\n",
        "4. [Spark MLlib](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) - de facto standard w pracy z wielkimi zbiorami danych, częstymi w systemach rekomendacyjnych. Implementuje explicit oraz implicit collaborative filtering.\n",
        "5. [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) - de facto standard dla grafowych sieci neuronowych (Graph Neural Networks, GNNs), które są m. in. najnowszym trendem w systemach rekomendacyjnych opartych o grafy (graph-based recommender systems).\n",
        "\n",
        "Dodatkowo dla podejścia content-based (opisane, ale nie implementowane w tym laboratorium) można użyć dowolnej biblioteki do uczenia nadzorowanego, typowo Scikit-learn lub Spark MLlib.\n",
        "\n",
        "Na tym laboratorium wykorzystamy `Surprise` ze względu na prostotę użycia. Dodatkowo użyjemy `recmetrics`, aby obliczyć metryki specyficzne dla systemów rekomendacyjnych, których nie implementuje Scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "K2PIvZpuiKQA",
        "outputId": "2de4cf8a-ca09-49b0-e865-319bb39325a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting recmetrics\n",
            "  Downloading recmetrics-0.1.5-py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Collecting funcsigs<2.0.0,>=1.0.2 (from recmetrics)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython<8.0.0,>=7.18.1 in /usr/local/lib/python3.10/dist-packages (from recmetrics) (7.34.0)\n",
            "Collecting jupyter<2.0.0,>=1.0.0 (from recmetrics)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from recmetrics) (3.10.0)\n",
            "Collecting pandas<2.0.0,>=1.1.3 (from recmetrics)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting plotly<5.0.0,>=4.11.0 (from recmetrics)\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pytest-cov<3.0.0,>=2.10.1 (from recmetrics)\n",
            "  Downloading pytest_cov-2.12.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from recmetrics) (1.6.0)\n",
            "Collecting seaborn<0.12.0,>=0.11.0 (from recmetrics)\n",
            "  Downloading seaborn-0.11.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting twine<5.0.0,>=4.0.0 (from recmetrics)\n",
            "  Downloading twine-4.0.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython<8.0.0,>=7.18.1->recmetrics)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.18.1->recmetrics) (4.9.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->recmetrics) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->recmetrics) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->recmetrics) (7.16.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->recmetrics) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->recmetrics) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyterlab-4.3.4-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.3.2->recmetrics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.1.3->recmetrics) (2024.2)\n",
            "Collecting retrying>=1.3.3 (from plotly<5.0.0,>=4.11.0->recmetrics)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from plotly<5.0.0,>=4.11.0->recmetrics) (1.17.0)\n",
            "Requirement already satisfied: pytest>=4.6 in /usr/local/lib/python3.10/dist-packages (from pytest-cov<3.0.0,>=2.10.1->recmetrics) (8.3.4)\n",
            "Collecting coverage>=5.2.1 (from pytest-cov<3.0.0,>=2.10.1->recmetrics)\n",
            "  Downloading coverage-7.6.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pytest-cov<3.0.0,>=2.10.1->recmetrics) (0.10.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->recmetrics) (3.5.0)\n",
            "Collecting pkginfo>=1.8.1 (from twine<5.0.0,>=4.0.0->recmetrics)\n",
            "  Downloading pkginfo-1.12.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine<5.0.0,>=4.0.0->recmetrics)\n",
            "  Downloading readme_renderer-44.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (1.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (2.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.10/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (8.5.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/lib/python3/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (23.5.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine<5.0.0,>=4.0.0->recmetrics)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from twine<5.0.0,>=4.0.0->recmetrics) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.6->twine<5.0.0,>=4.0.0->recmetrics) (3.21.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.18.1->recmetrics) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.18.1->recmetrics) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.18.1->recmetrics) (0.2.13)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.6->pytest-cov<3.0.0,>=2.10.1->recmetrics) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=4.6->pytest-cov<3.0.0,>=2.10.1->recmetrics) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.6->pytest-cov<3.0.0,>=2.10.1->recmetrics) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.6->pytest-cov<3.0.0,>=2.10.1->recmetrics) (2.2.1)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine<5.0.0,>=4.0.0->recmetrics)\n",
            "  Downloading nh3-0.2.20-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: docutils>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine<5.0.0,>=4.0.0->recmetrics) (0.21.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine<5.0.0,>=4.0.0->recmetrics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine<5.0.0,>=4.0.0->recmetrics) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine<5.0.0,>=4.0.0->recmetrics) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->twine<5.0.0,>=4.0.0->recmetrics) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->twine<5.0.0,>=4.0.0->recmetrics) (4.12.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter<2.0.0,>=1.0.0->recmetrics) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter<2.0.0,>=1.0.0->recmetrics) (3.0.13)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (0.28.1)\n",
            "Collecting ipykernel (from jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (0.2.4)\n",
            "Collecting comm>=0.1.1 (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->recmetrics) (24.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (3.1.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (1.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (0.14.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (4.3.6)\n",
            "Collecting jupyter-client (from jupyter-console->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyter_events-0.11.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client->jupyter-console->jupyter<2.0.0,>=1.0.0->recmetrics) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (4.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine<5.0.0,>=4.0.0->recmetrics) (0.1.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (2.21.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter<2.0.0,>=1.0.0->recmetrics) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (0.22.3)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->recmetrics) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter<2.0.0,>=1.0.0->recmetrics)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading recmetrics-0.1.5-py3-none-any.whl (9.3 kB)\n",
            "Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_cov-2.12.1-py2.py3-none-any.whl (20 kB)\n",
            "Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twine-4.0.2-py3-none-any.whl (36 kB)\n",
            "Downloading coverage-7.6.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.12.0-py3-none-any.whl (32 kB)\n",
            "Downloading readme_renderer-44.0-py3-none-any.whl (13 kB)\n",
            "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading jupyterlab-4.3.4-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.20-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (744 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m744.4/744.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.11.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357284 sha256=d50f94cf4e3da956d4826b91918a93545bde51ba8d49df5459b52624c786ceb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: funcsigs, uri-template, types-python-dateutil, rfc3986-validator, rfc3986, rfc3339-validator, retrying, python-json-logger, pkginfo, overrides, nh3, json5, jedi, fqdn, coverage, comm, async-lru, scikit-surprise, readme-renderer, pytest-cov, plotly, pandas, jupyter-server-terminals, jupyter-client, arrow, twine, seaborn, isoduration, ipykernel, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, recmetrics\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 coverage-7.6.10 fqdn-1.5.1 funcsigs-1.0.2 ipykernel-6.29.5 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.11.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.4 jupyterlab-server-2.27.3 nh3-0.2.20 overrides-7.7.0 pandas-1.5.3 pkginfo-1.12.0 plotly-4.14.3 pytest-cov-2.12.1 python-json-logger-3.2.1 readme-renderer-44.0 recmetrics-0.1.5 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-2.0.0 rfc3986-validator-0.1.1 scikit-surprise-1.1.4 seaborn-0.11.2 twine-4.0.2 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise recmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN6tvFVeiKQA"
      },
      "source": [
        "## Ściąganie, ładowanie i eksploracja danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4vFmWxAiKQB"
      },
      "source": [
        "Na początek ściągniemy nasz zbiór danych. Będziemy pracować na zbiorze MovieLens-100k, czyli zbiorze 100 tysięcy ocen filmów. Strona MovieLens udostępnia wiele rozmiarów tego zbioru danych, a ten będzie odpowiednio szybki na potrzeby edukacyjne. W praktyce wykorzystuje się zbiory rozmiaru co najmniej takiego, jak MovieLens-1M (zbiór miliona ocen).\n",
        "\n",
        "Opis plików można znaleźć w [readme](https://files.grouplens.org/datasets/movielens/ml-100k-README.txt). Najważniejsze fragmenty:\n",
        "```\n",
        "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
        "              Each user has rated at least 20 movies.  Users and items are\n",
        "              numbered consecutively from 1.  The data is randomly\n",
        "              ordered. This is a tab separated list of\n",
        "\t         user id | item id | rating | timestamp.\n",
        "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
        "```\n",
        "\n",
        "Zbiór co prawda ma już przygotowany podział do 5-krotnej walidacji skrośnej (pliki `u1.base`, `u1.test` etc.), ale my wykonamy ten podział sami. Gotowych podziałów używa się w pracach naukowych, aby móc porównywać wyniki różnych algorytmów na dokładnie tych samych zbiorach treningowych i testowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qNq3BpAviKQB",
        "outputId": "2778eb88-3da3-471c-ca4d-f6a035641f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-14 16:47:44--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  8.95MB/s    in 0.5s    \n",
            "\n",
            "2025-01-14 16:47:45 (8.95 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N https://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "wWmaNXUwiKQB",
        "outputId": "7d225def-d394-4606-e821-1e9995ca749c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ]
        }
      ],
      "source": [
        "!unzip -n ml-100k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OCqon-AoiKQB",
        "outputId": "ba7c9536-fcc6-40d2-e976-fe79f63f5b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id  item_id  rating  timestamp\n",
              "0      196      242       3  881250949\n",
              "1      186      302       3  891717742\n",
              "2       22      377       1  878887116\n",
              "3      244       51       2  880606923\n",
              "4      166      346       1  886397596"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f57c936-cbb7-4ec7-b6fa-dc5aba185d59\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f57c936-cbb7-4ec7-b6fa-dc5aba185d59')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f57c936-cbb7-4ec7-b6fa-dc5aba185d59 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f57c936-cbb7-4ec7-b6fa-dc5aba185d59');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2d2145c5-3d71-4fa1-a688-47f9c290f882\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2d2145c5-3d71-4fa1-a688-47f9c290f882')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2d2145c5-3d71-4fa1-a688-47f9c290f882 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 266,\n        \"min\": 1,\n        \"max\": 943,\n        \"num_unique_values\": 943,\n        \"samples\": [\n          262,\n          136,\n          821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 330,\n        \"min\": 1,\n        \"max\": 1682,\n        \"num_unique_values\": 1682,\n        \"samples\": [\n          1557,\n          808,\n          1618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5343856,\n        \"min\": 874724710,\n        \"max\": 893286638,\n        \"num_unique_values\": 49282,\n        \"samples\": [\n          889728713,\n          888443306,\n          880605158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    os.path.join(\"ml-100k\", \"u.data\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4u_Ogo4hiKQB",
        "outputId": "50adb718-8146-4f14-d35b-40197aa3c424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews: 100000\n",
            "Ratings range: (1, 5)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of reviews: {len(df)}\")\n",
        "print(f\"Ratings range: {df.rating.min(), df.rating.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk4GOWaxiKQB"
      },
      "source": [
        "Tabela w formacie jak powyżej to de facto słownik `(user_id, item_id) -> rating`. Jest zatem idealna do podejścia collaborative filtering, w którym dla **użytkowników (users)** mamy ich **oceny (ratings)** wybranych **przedmiotów (items)**. Tutaj oczywiście przedmiotami są filmy. Można by zatem z takich danych zbudować **macierz ocen (ratings matrix)**, w której wiersze byłyby użytkownikami, kolumny przedmiotami, a komórki zawierałyby oceny.\n",
        "\n",
        "![Rating-matrix-representation-of-recommendation-data.png](attachment:Rating-matrix-representation-of-recommendation-data.png)\n",
        "\n",
        "W przyszłości chcemy zatem **przewidywać wartości brakujące** macierzy ocen. Mamy tu zatem poniekąd problem regresji - chcemy dostać wartość ciągłą, np. na ile użytkownik oceniłby film, którego jeszcze nie widział. Późniejsza rekomendacja to po prostu wybranie najwyższych predykcji i zaproponowanie tych właśnie przedmiotów. Nazywa się to czasem problemem **uzupełnienia macierzy (matrix completion)**.\n",
        "\n",
        "W praktyce **nigdy** nie należy budować macierzy ocen explicite. Zwyczajnie nie zmieściłaby się ona do pamięci dla zbiorów o prawdziwym rozmiarze, kiedy mamy setki tysięcy użytkowników i przedmiotów. Dodatkowo zwyczajnie nie ma to sensu, bo nasze macierze prawie zawsze są **rzadkie (sparse)**, tzn. mają wypełnioną tylko nieznaczną liczbę pól. Reszta jest nieznana - w końcu pojedynczy człowiek obejrzy tylko niewielką część wszystkich filmów z Netflixa, nie mówiąc już o wystawieniu im ocen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04WhslEriKQB"
      },
      "source": [
        "**Zadanie 1 (0.25 punktu)**\n",
        "\n",
        "Oblicz gęstość (density) macierzy ocen dla naszego zbioru danych. Jest to liczba ocen, podzielona przez rozmiar macierzy ocen (liczba użytkowników * liczba przedmiotów). Wynik przedstaw w procentach, zaokrąglony do 4 miejsc po przecinku. Pamiętaj, żeby uwzględnić tylko unikatowych użytkowników i przedmioty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_SbUxdqXiKQC",
        "outputId": "fb644c1e-c91b-4f5b-dd5d-32634862ac97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Density: 6.3047%\n"
          ]
        }
      ],
      "source": [
        "review_count = df.shape[0]\n",
        "user_count = df[\"user_id\"].nunique()\n",
        "item_count = df[\"item_id\"].nunique()\n",
        "\n",
        "density = review_count / (user_count * item_count)\n",
        "print(f\"Density: {100 * density:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQloWufliKQC"
      },
      "source": [
        "W praktyce często zbiory mają gęstość rzędu 1% lub mniejszą. Jest to też pozytywne - w końcu to dzięki temu mamy komu robić rekomendacje (i czego).\n",
        "\n",
        "Warto zauważyć, że nasz zbiór zawiera tylko tych użytkowników, którzy ocenili przynajmniej 20 filmów, a zatem wiemy o każdym z nich całkiem sporo. Unika to problemu **zimnego startu (cold start)**, w którym nic nie wiemy o nowych użytkownikach i/lub filmach. W prawdziwych systemach jest to jednak duże wyzwanie. Można sobie z nim radzić na kilka sposobów:\n",
        "- rekomendować najpopularniejsze przedmioty\n",
        "- rekomendować przedmioty o najwyższych ocenach\n",
        "- użyć globalnego (niepersonalizowanego) systemu rekomendacyjnego, np. przewidywanie średniej dla przedmiotu\n",
        "- używać systemu content-based, bo radzą sobie dobrze przy małej liczbie interakcji\n",
        "- poprosić użytkownika przy pierwszym logowaniu o podanie pierwszych preferencji (nie zawsze możliwe)\n",
        "\n",
        "W związku z problemem zimnego startu systemy rekomendacyjne zwykle są (co najmniej) dwuetapowe i mają osobny algorytm dla nowych użytkowników/przedmiotów oraz osobny dla tych, o których już coś wiemy więcej i możemy dokonywać personalizacji.\n",
        "\n",
        "Zbadajmy teraz rozkład popularności poszczególnych przedmiotów w naszym zbiorze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrqgI_qiKQC"
      },
      "source": [
        "**Zadanie 2 (0.25 punktu)**\n",
        "\n",
        "Narysuj wykres popularności (liczby ocen) dla poszczególnych przedmiotów. Użyj odpowiednio dużej liczby kubełków histogramu, żeby zwizualizować kształt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "li2sQOVKiKQC",
        "outputId": "e9e9798d-2f3e-4de8-f9ec-bf8909a21057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGdCAYAAADT1TPdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJYBJREFUeJzt3X9wlPWBx/FPQpIl/NiNCWSXnATTkRZSfkgTCVvs/ZA9IqaenrGnTmrTHiMjl1ghlkruEJT2DIN3anGUnF4L3FSOKzeHVShgGmqosgRI5RrARqx4ScVNuHLZBVryg3zvD4/nuoLChpD9srxfM8+MeZ7vbr7Pd8B9z7PPLknGGCMAAABLJcd7AgAAAJ+GWAEAAFYjVgAAgNWIFQAAYDViBQAAWI1YAQAAViNWAACA1YgVAABgtZR4T6A/+vr6dPToUY0cOVJJSUnxng4AALgIxhidOHFCOTk5Sk6++OslV2SsHD16VGPHjo33NAAAQD+0tbXp2muvvejxV2SsjBw5UtJHJ+t2u+M8GwAAcDEikYjGjh3rvI5frCsyVs6+9eN2u4kVAACuMLHewsENtgAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsFpKvCdgo+sWb7ngmPdXlAzCTAAAAFdWAACA1WKOlQ8++EBf/epXlZWVpfT0dE2ePFn79u1zjhtjtHTpUo0ZM0bp6ekKBAI6fPhw1HMcP35cZWVlcrvdysjI0Ny5c3Xy5MlLPxsAAJBwYoqV//mf/9HMmTOVmpqqrVu36tChQ/rHf/xHXXPNNc6YlStXatWqVaqtrVVjY6OGDx+u4uJinT592hlTVlamgwcPqq6uTps3b9bOnTs1b968gTsrAACQMJKMMeZiBy9evFhvvvmmfv7zn5/3uDFGOTk5evjhh/Wtb31LkhQOh+X1erV27Vrdc889evvtt5Wfn6+9e/eqsLBQkrRt2zbdeuut+s1vfqOcnJwLziMSicjj8SgcDsvtdl/s9C8a96wAADDw+vv6HdOVlVdeeUWFhYX6yle+ouzsbE2bNk0vvviic/zIkSMKhUIKBALOPo/Ho6KiIgWDQUlSMBhURkaGEyqSFAgElJycrMbGxlimAwAArgIxxcp7772n1atXa/z48dq+fbvmz5+vb37zm1q3bp0kKRQKSZK8Xm/U47xer3MsFAopOzs76nhKSooyMzOdMR/X1dWlSCQStQEAgKtDTB9d7uvrU2FhoZ544glJ0rRp03TgwAHV1taqvLz8skxQkmpqavT4449ftucHAAD2iunKypgxY5Sfnx+1b+LEiWptbZUk+Xw+SVJ7e3vUmPb2dueYz+dTR0dH1PHe3l4dP37cGfNx1dXVCofDztbW1hbLtAEAwBUspliZOXOmWlpaova98847GjdunCQpLy9PPp9P9fX1zvFIJKLGxkb5/X5Jkt/vV2dnp5qampwxO3bsUF9fn4qKis77e10ul9xud9QGAACuDjG9DbRw4UJ98Ytf1BNPPKG/+qu/0p49e/TCCy/ohRdekCQlJSVpwYIF+u53v6vx48crLy9Pjz76qHJycnTHHXdI+uhKzC233KL7779ftbW16unpUWVlpe65556L+iQQAAC4usQUKzfeeKM2bdqk6upqLV++XHl5eXrmmWdUVlbmjPn2t7+tU6dOad68eers7NRNN92kbdu2aejQoc6Yl156SZWVlZo1a5aSk5NVWlqqVatWDdxZAQCAhBHT96zYgu9ZAQDgyjMo37MCAAAw2IgVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGA1YgUAAFiNWAEAAFYjVgAAgNWIFQAAYDViBQAAWI1YAQAAViNWAACA1YgVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGA1YgUAAFiNWAEAAFYjVgAAgNWIFQAAYDViBQAAWI1YAQAAViNWAACA1YgVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGA1YgUAAFiNWAEAAFYjVgAAgNWIFQAAYDViBQAAWI1YAQAAViNWAACA1YgVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGA1YgUAAFiNWAEAAFYjVgAAgNWIFQAAYDViBQAAWC2mWHnssceUlJQUtU2YMME5fvr0aVVUVCgrK0sjRoxQaWmp2tvbo56jtbVVJSUlGjZsmLKzs7Vo0SL19vYOzNkAAICEkxLrAz7/+c/rpz/96f8/Qcr/P8XChQu1ZcsWbdy4UR6PR5WVlbrzzjv15ptvSpLOnDmjkpIS+Xw+7dq1Sx9++KG+9rWvKTU1VU888cQAnA4AAEg0McdKSkqKfD7fOfvD4bC+//3va/369br55pslSWvWrNHEiRO1e/duzZgxQ6+99poOHTqkn/70p/J6vbrhhhv0ne98R4888ogee+wxpaWlXfoZAQCAhBLzPSuHDx9WTk6OPvOZz6isrEytra2SpKamJvX09CgQCDhjJ0yYoNzcXAWDQUlSMBjU5MmT5fV6nTHFxcWKRCI6ePDgJ/7Orq4uRSKRqA0AAFwdYoqVoqIirV27Vtu2bdPq1at15MgRfelLX9KJEycUCoWUlpamjIyMqMd4vV6FQiFJUigUigqVs8fPHvskNTU18ng8zjZ27NhYpg0AAK5gMb0NNGfOHOe/p0yZoqKiIo0bN04/+tGPlJ6ePuCTO6u6ulpVVVXOz5FIhGABAOAqcUkfXc7IyNBnP/tZvfvuu/L5fOru7lZnZ2fUmPb2duceF5/Pd86ng87+fL77YM5yuVxyu91RGwAAuDpcUqycPHlSv/71rzVmzBgVFBQoNTVV9fX1zvGWlha1trbK7/dLkvx+v5qbm9XR0eGMqaurk9vtVn5+/qVMBQAAJKiY3gb61re+pdtuu03jxo3T0aNHtWzZMg0ZMkT33nuvPB6P5s6dq6qqKmVmZsrtduvBBx+U3+/XjBkzJEmzZ89Wfn6+7rvvPq1cuVKhUEhLlixRRUWFXC7XZTlBAABwZYspVn7zm9/o3nvv1W9/+1uNHj1aN910k3bv3q3Ro0dLkp5++mklJyertLRUXV1dKi4u1vPPP+88fsiQIdq8ebPmz58vv9+v4cOHq7y8XMuXLx/YswIAAAkjyRhj4j2JWEUiEXk8HoXD4cty/8p1i7dccMz7K0oG/PcCAJDI+vv6zb8NBAAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGqXFCsrVqxQUlKSFixY4Ow7ffq0KioqlJWVpREjRqi0tFTt7e1Rj2ttbVVJSYmGDRum7OxsLVq0SL29vZcyFQAAkKD6HSt79+7VP/3TP2nKlClR+xcuXKhXX31VGzduVENDg44ePao777zTOX7mzBmVlJSou7tbu3bt0rp167R27VotXbq0/2cBAAASVr9i5eTJkyorK9OLL76oa665xtkfDof1/e9/X0899ZRuvvlmFRQUaM2aNdq1a5d2794tSXrttdd06NAh/fCHP9QNN9ygOXPm6Dvf+Y6ee+45dXd3D8xZAQCAhNGvWKmoqFBJSYkCgUDU/qamJvX09ETtnzBhgnJzcxUMBiVJwWBQkydPltfrdcYUFxcrEono4MGD5/19XV1dikQiURsAALg6pMT6gA0bNugXv/iF9u7de86xUCiktLQ0ZWRkRO33er0KhULOmD8MlbPHzx47n5qaGj3++OOxThUAACSAmK6stLW16aGHHtJLL72koUOHXq45naO6ulrhcNjZ2traBu13AwCA+IopVpqamtTR0aEvfOELSklJUUpKihoaGrRq1SqlpKTI6/Wqu7tbnZ2dUY9rb2+Xz+eTJPl8vnM+HXT257NjPs7lcsntdkdtAADg6hBTrMyaNUvNzc3av3+/sxUWFqqsrMz579TUVNXX1zuPaWlpUWtrq/x+vyTJ7/erublZHR0dzpi6ujq53W7l5+cP0GkBAIBEEdM9KyNHjtSkSZOi9g0fPlxZWVnO/rlz56qqqkqZmZlyu9168MEH5ff7NWPGDEnS7NmzlZ+fr/vuu08rV65UKBTSkiVLVFFRIZfLNUCnBQAAEkXMN9heyNNPP63k5GSVlpaqq6tLxcXFev75553jQ4YM0ebNmzV//nz5/X4NHz5c5eXlWr58+UBPBQAAJIAkY4yJ9yRiFYlE5PF4FA6HL8v9K9ct3nLBMe+vKBnw3wsAQCLr7+s3/zYQAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArBZTrKxevVpTpkyR2+2W2+2W3+/X1q1bneOnT59WRUWFsrKyNGLECJWWlqq9vT3qOVpbW1VSUqJhw4YpOztbixYtUm9v78CcDQAASDgxxcq1116rFStWqKmpSfv27dPNN9+s22+/XQcPHpQkLVy4UK+++qo2btyohoYGHT16VHfeeafz+DNnzqikpETd3d3atWuX1q1bp7Vr12rp0qUDe1YAACBhJBljzKU8QWZmpp588kndddddGj16tNavX6+77rpLkvSrX/1KEydOVDAY1IwZM7R161Z9+ctf1tGjR+X1eiVJtbW1euSRR3Ts2DGlpaVd1O+MRCLyeDwKh8Nyu92XMv3zum7xlguOeX9FyYD/XgAAEll/X7/7fc/KmTNntGHDBp06dUp+v19NTU3q6elRIBBwxkyYMEG5ubkKBoOSpGAwqMmTJzuhIknFxcWKRCLO1Znz6erqUiQSidoAAMDVIeZYaW5u1ogRI+RyufTAAw9o06ZNys/PVygUUlpamjIyMqLGe71ehUIhSVIoFIoKlbPHzx77JDU1NfJ4PM42duzYWKcNAACuUDHHyuc+9znt379fjY2Nmj9/vsrLy3Xo0KHLMTdHdXW1wuGws7W1tV3W3wcAAOyREusD0tLSdP3110uSCgoKtHfvXn3ve9/T3Xffre7ubnV2dkZdXWlvb5fP55Mk+Xw+7dmzJ+r5zn5a6OyY83G5XHK5XLFOFQAAJIBL/p6Vvr4+dXV1qaCgQKmpqaqvr3eOtbS0qLW1VX6/X5Lk9/vV3Nysjo4OZ0xdXZ3cbrfy8/MvdSoAACABxXRlpbq6WnPmzFFubq5OnDih9evX6/XXX9f27dvl8Xg0d+5cVVVVKTMzU263Ww8++KD8fr9mzJghSZo9e7by8/N13333aeXKlQqFQlqyZIkqKiq4cgIAAM4rpljp6OjQ1772NX344YfyeDyaMmWKtm/frj//8z+XJD399NNKTk5WaWmpurq6VFxcrOeff955/JAhQ7R582bNnz9ffr9fw4cPV3l5uZYvXz6wZwUAABLGJX/PSjzwPSsAAFx5Bv17VgAAAAYDsQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrxRQrNTU1uvHGGzVy5EhlZ2frjjvuUEtLS9SY06dPq6KiQllZWRoxYoRKS0vV3t4eNaa1tVUlJSUaNmyYsrOztWjRIvX29l762QAAgIQTU6w0NDSooqJCu3fvVl1dnXp6ejR79mydOnXKGbNw4UK9+uqr2rhxoxoaGnT06FHdeeedzvEzZ86opKRE3d3d2rVrl9atW6e1a9dq6dKlA3dWAAAgYSQZY0x/H3zs2DFlZ2eroaFBf/zHf6xwOKzRo0dr/fr1uuuuuyRJv/rVrzRx4kQFg0HNmDFDW7du1Ze//GUdPXpUXq9XklRbW6tHHnlEx44dU1pa2gV/byQSkcfjUTgcltvt7u/0P9F1i7dccMz7K0oG/PcCAJDI+vv6fUn3rITDYUlSZmamJKmpqUk9PT0KBALOmAkTJig3N1fBYFCSFAwGNXnyZCdUJKm4uFiRSEQHDx487+/p6upSJBKJ2gAAwNWh37HS19enBQsWaObMmZo0aZIkKRQKKS0tTRkZGVFjvV6vQqGQM+YPQ+Xs8bPHzqempkYej8fZxo4d299pAwCAK0xKfx9YUVGhAwcO6I033hjI+ZxXdXW1qqqqnJ8jkUjcg4W3igAAGBz9ipXKykpt3rxZO3fu1LXXXuvs9/l86u7uVmdnZ9TVlfb2dvl8PmfMnj17op7v7KeFzo75OJfLJZfL1Z+pAgCAK1xMbwMZY1RZWalNmzZpx44dysvLizpeUFCg1NRU1dfXO/taWlrU2toqv98vSfL7/WpublZHR4czpq6uTm63W/n5+ZdyLgAAIAHFdGWloqJC69ev149//GONHDnSucfE4/EoPT1dHo9Hc+fOVVVVlTIzM+V2u/Xggw/K7/drxowZkqTZs2crPz9f9913n1auXKlQKKQlS5aooqKCqycAAOAcMcXK6tWrJUl/+qd/GrV/zZo1+vrXvy5Jevrpp5WcnKzS0lJ1dXWpuLhYzz//vDN2yJAh2rx5s+bPny+/36/hw4ervLxcy5cvv7QzAQAACemSvmclXmz4npWLwQ22AAD8v7h8zwoAAMDlRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGrECAACsRqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAq6XEewKJ7LrFWy445v0VJYMwEwAArlxcWQEAAFYjVgAAgNV4GyjOeKsIAIBPx5UVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGA1YgUAAFiNWAEAAFYjVgAAgNVijpWdO3fqtttuU05OjpKSkvTyyy9HHTfGaOnSpRozZozS09MVCAR0+PDhqDHHjx9XWVmZ3G63MjIyNHfuXJ08efKSTgQAACSmmGPl1KlTmjp1qp577rnzHl+5cqVWrVql2tpaNTY2avjw4SouLtbp06edMWVlZTp48KDq6uq0efNm7dy5U/Pmzev/WQAAgIQV89ftz5kzR3PmzDnvMWOMnnnmGS1ZskS33367JOlf/uVf5PV69fLLL+uee+7R22+/rW3btmnv3r0qLCyUJD377LO69dZb9Q//8A/Kycm5hNMBAACJZkDvWTly5IhCoZACgYCzz+PxqKioSMFgUJIUDAaVkZHhhIokBQIBJScnq7GxcSCnAwAAEsCA/kOGoVBIkuT1eqP2e71e51goFFJ2dnb0JFJSlJmZ6Yz5uK6uLnV1dTk/RyKRgZw2AACw2BXxaaCamhp5PB5nGzt2bLynBAAABsmAxorP55Mktbe3R+1vb293jvl8PnV0dEQd7+3t1fHjx50xH1ddXa1wOOxsbW1tAzltAABgsQGNlby8PPl8PtXX1zv7IpGIGhsb5ff7JUl+v1+dnZ1qampyxuzYsUN9fX0qKio67/O6XC653e6oDQAAXB1ivmfl5MmTevfdd52fjxw5ov379yszM1O5ublasGCBvvvd72r8+PHKy8vTo48+qpycHN1xxx2SpIkTJ+qWW27R/fffr9raWvX09KiyslL33HMPnwQCAADniDlW9u3bpz/7sz9zfq6qqpIklZeXa+3atfr2t7+tU6dOad68eers7NRNN92kbdu2aejQoc5jXnrpJVVWVmrWrFlKTk5WaWmpVq1aNQCnAwAAEk2SMcbEexKxikQi8ng8CofDl+UtoesWbxnw57wU768oifcUAAC4ZP19/b4iPg0EAACuXsQKAACwGrECAACsNqDfYIvL42LuoeG+FgBAouLKCgAAsBqxAgAArEasAAAAqxErAADAatxgmyC4CRcAkKi4sgIAAKxGrAAAAKsRKwAAwGrECgAAsBqxAgAArEasAAAAqxErAADAasQKAACwGl8KdxXhi+MAAFcirqwAAACrESsAAMBqxAoAALAasQIAAKxGrAAAAKvxaSBE4RNDAADbcGUFAABYjSsriBuu4gAALgZXVgAAgNWIFQAAYDViBQAAWI1YAQAAVuMGW8SMG2MBAIOJKysAAMBqxAoAALAasQIAAKxGrAAAAKtxgy0ui4u5Cdc23DgMAHbiygoAALAasQIAAKzG20CwGm/NAACIFVzxCBoASGzECq4KV+INvwCAjxArQAwG8yoOV4wA4CPcYAsAAKxGrAAAAKvxNhAwwLg/5tLxFhiAP0SsACAOAFiNWAESHFd6AFzp4horzz33nJ588kmFQiFNnTpVzz77rKZPnx7PKQFXlMEMEa6+AIiXuMXKv/3bv6mqqkq1tbUqKirSM888o+LiYrW0tCg7Ozte0wJwCWwLGtvmA6B/4hYrTz31lO6//3594xvfkCTV1tZqy5Yt+sEPfqDFixfHa1oALjPelgIQq7jESnd3t5qamlRdXe3sS05OViAQUDAYPGd8V1eXurq6nJ/D4bAkKRKJXJb59XX97rI8L4CBczF//y/m73Luwo0DMR0deLz4gmMmLds+IL/rYgzmfC7mdw2mizmvgVof287ddmf/3hpjYnugiYMPPvjASDK7du2K2r9o0SIzffr0c8YvW7bMSGJjY2NjY2NLgK2trS2mbrgiPg1UXV2tqqoq5+e+vj4dP35cWVlZSkpKGrDfE4lENHbsWLW1tcntdg/Y8yYy1qx/WLfYsWb9w7rFjjXrn4tZN2OMTpw4oZycnJieOy6xMmrUKA0ZMkTt7e1R+9vb2+Xz+c4Z73K55HK5ovZlZGRctvm53W7+gMaINesf1i12rFn/sG6xY83650Lr5vF4Yn7OuHzdflpamgoKClRfX+/s6+vrU319vfx+fzymBAAALBW3t4GqqqpUXl6uwsJCTZ8+Xc8884xOnTrlfDoIAABAimOs3H333Tp27JiWLl2qUCikG264Qdu2bZPX643XlORyubRs2bJz3nLCJ2PN+od1ix1r1j+sW+xYs/65nOuWZEysnx8CAAAYPHG5ZwUAAOBiESsAAMBqxAoAALAasQIAAKxGrPyf5557Ttddd52GDh2qoqIi7dmzJ95TiqudO3fqtttuU05OjpKSkvTyyy9HHTfGaOnSpRozZozS09MVCAR0+PDhqDHHjx9XWVmZ3G63MjIyNHfuXJ08eXIQz2Jw1dTU6MYbb9TIkSOVnZ2tO+64Qy0tLVFjTp8+rYqKCmVlZWnEiBEqLS0958sRW1tbVVJSomHDhik7O1uLFi1Sb2/vYJ7KoFm9erWmTJnifImU3+/X1q1bneOs14WtWLFCSUlJWrBggbOPdTvXY489pqSkpKhtwoQJznHW7JN98MEH+upXv6qsrCylp6dr8uTJ2rdvn3N8UF4P+vvv+ySSDRs2mLS0NPODH/zAHDx40Nx///0mIyPDtLe3x3tqcfOTn/zE/N3f/Z35j//4DyPJbNq0Ker4ihUrjMfjMS+//LL5z//8T/MXf/EXJi8vz/z+9793xtxyyy1m6tSpZvfu3ebnP/+5uf7668299947yGcyeIqLi82aNWvMgQMHzP79+82tt95qcnNzzcmTJ50xDzzwgBk7dqypr683+/btMzNmzDBf/OIXneO9vb1m0qRJJhAImLfeesv85Cc/MaNGjTLV1dXxOKXL7pVXXjFbtmwx77zzjmlpaTF/+7d/a1JTU82BAweMMazXhezZs8dcd911ZsqUKeahhx5y9rNu51q2bJn5/Oc/bz788ENnO3bsmHOcNTu/48ePm3Hjxpmvf/3rprGx0bz33ntm+/bt5t1333XGDMbrAbFijJk+fbqpqKhwfj5z5ozJyckxNTU1cZyVPT4eK319fcbn85knn3zS2dfZ2WlcLpf513/9V2OMMYcOHTKSzN69e50xW7duNUlJSeaDDz4YtLnHU0dHh5FkGhoajDEfrVFqaqrZuHGjM+btt982kkwwGDTGfBSJycnJJhQKOWNWr15t3G636erqGtwTiJNrrrnG/PM//zPrdQEnTpww48ePN3V1deZP/uRPnFhh3c5v2bJlZurUqec9xpp9skceecTcdNNNn3h8sF4Prvq3gbq7u9XU1KRAIODsS05OViAQUDAYjOPM7HXkyBGFQqGoNfN4PCoqKnLWLBgMKiMjQ4WFhc6YQCCg5ORkNTY2Dvqc4yEcDkuSMjMzJUlNTU3q6emJWrcJEyYoNzc3at0mT54c9eWIxcXFikQiOnjw4CDOfvCdOXNGGzZs0KlTp+T3+1mvC6ioqFBJSUnU+kj8Ofs0hw8fVk5Ojj7zmc+orKxMra2tklizT/PKK6+osLBQX/nKV5Sdna1p06bpxRdfdI4P1uvBVR8r//3f/60zZ86c8825Xq9XoVAoTrOy29l1+bQ1C4VCys7OjjqekpKizMzMq2Jd+/r6tGDBAs2cOVOTJk2S9NGapKWlnfOPcH583c63rmePJaLm5maNGDFCLpdLDzzwgDZt2qT8/HzW61Ns2LBBv/jFL1RTU3POMdbt/IqKirR27Vpt27ZNq1ev1pEjR/SlL31JJ06cYM0+xXvvvafVq1dr/Pjx2r59u+bPn69vfvObWrdunaTBez2I29ftA4msoqJCBw4c0BtvvBHvqVjvc5/7nPbv369wOKx///d/V3l5uRoaGuI9LWu1tbXpoYceUl1dnYYOHRrv6Vwx5syZ4/z3lClTVFRUpHHjxulHP/qR0tPT4zgzu/X19amwsFBPPPGEJGnatGk6cOCAamtrVV5ePmjzuOqvrIwaNUpDhgw5567v9vZ2+Xy+OM3KbmfX5dPWzOfzqaOjI+p4b2+vjh8/nvDrWllZqc2bN+tnP/uZrr32Wme/z+dTd3e3Ojs7o8Z/fN3Ot65njyWitLQ0XX/99SooKFBNTY2mTp2q733ve6zXJ2hqalJHR4e+8IUvKCUlRSkpKWpoaNCqVauUkpIir9fLul2EjIwMffazn9W7777Ln7VPMWbMGOXn50ftmzhxovMW2mC9Hlz1sZKWlqaCggLV19c7+/r6+lRfXy+/3x/HmdkrLy9PPp8vas0ikYgaGxudNfP7/ers7FRTU5MzZseOHerr61NRUdGgz3kwGGNUWVmpTZs2aceOHcrLy4s6XlBQoNTU1Kh1a2lpUWtra9S6NTc3R/3Frqurk9vtPud/GImqr69PXV1drNcnmDVrlpqbm7V//35nKywsVFlZmfPfrNuFnTx5Ur/+9a81ZswY/qx9ipkzZ57zFQzvvPOOxo0bJ2kQXw/6d39wYtmwYYNxuVxm7dq15tChQ2bevHkmIyMj6q7vq82JEyfMW2+9Zd566y0jyTz11FPmrbfeMv/1X/9ljPnoo2oZGRnmxz/+sfnlL39pbr/99vN+VG3atGmmsbHRvPHGG2b8+PEJ/dHl+fPnG4/HY15//fWoj0f+7ne/c8Y88MADJjc31+zYscPs27fP+P1+4/f7neNnPx45e/Zss3//frNt2zYzevTohP145OLFi01DQ4M5cuSI+eUvf2kWL15skpKSzGuvvWaMYb0u1h9+GsgY1u18Hn74YfP666+bI0eOmDfffNMEAgEzatQo09HRYYxhzT7Jnj17TEpKivn7v/97c/jwYfPSSy+ZYcOGmR/+8IfOmMF4PSBW/s+zzz5rcnNzTVpampk+fbrZvXt3vKcUVz/72c+MpHO28vJyY8xHH1d79NFHjdfrNS6Xy8yaNcu0tLREPcdvf/tbc++995oRI0YYt9ttvvGNb5gTJ07E4WwGx/nWS5JZs2aNM+b3v/+9+Zu/+RtzzTXXmGHDhpm//Mu/NB9++GHU87z//vtmzpw5Jj093YwaNco8/PDDpqenZ5DPZnD89V//tRk3bpxJS0szo0ePNrNmzXJCxRjW62J9PFZYt3PdfffdZsyYMSYtLc380R/9kbn77rujviuENftkr776qpk0aZJxuVxmwoQJ5oUXXog6PhivB0nGGNOPK0MAAACD4qq/ZwUAANiNWAEAAFYjVgAAgNWIFQAAYDViBQAAWI1YAQAAViNWAACA1YgVAABgNWIFAABYjVgBAABWI1YAAIDViBUAAGC1/wWhlg+4ZDuGmwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(df[\"item_id\"].value_counts(), bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An0T2MCBiKQC"
      },
      "source": [
        "Typowo niewielka liczba przedmiotów odpowiada za większość ocen. Są to rzeczy bardzo znane i popularne, napędzane efektem kuli śnieżnej. Przykładowo, \"Titanic\" ogląda i ocenia bardzo znaczna liczba użytkowników, przez sam fakt, jak bardzo znany jest ten film. My jesteśmy zwykle zainteresowani **długim ogonem (long tail)** naszego rozkładu popularności, czyli zwiększeniem popularności tych przedmiotów, które są mniej znane, a które możemy zaoferować użytkownikom, np. nowa muzyka do odkrycia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBrqKaXiKQC"
      },
      "source": [
        "**Eksploracja danych - podsumowanie**\n",
        "\n",
        "1. W systemach typu collaborative filtering operujemy na macierzy ocen, gdzie wierszami są użytkownicy, kolumnami oceny, a w komórkach znajdują się oceny.\n",
        "2. Macierz ocen jest zwykle bardzo rzadka.\n",
        "3. Kiedy niewiele wiemy o użytkowniku lub przedmiocie, to mamy problem zimnego startu, z którym trzeba sobie w jakiś sposób poradzić.\n",
        "4. Często występuje zjawisko długiego ogona, czyli dominacji niewielkiej grupy bardzo popularnych przedmiotów."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odE0S_peiKQC"
      },
      "source": [
        "## Walidacja modeli, prosty model bazowy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8yZTDb8iKQC"
      },
      "source": [
        "Na początek, zanim zaczniemy budować nasze modele, trzeba wyodrębnić zbiór testowy. Mamy tutaj kilka możliwości. Po pierwsze, można po prostu losowo, tak jak to robiliśmy do tej pory, i tak bardzo często się robi.\n",
        "\n",
        "Zbiór testowy ma jednak symulować przyszłe dane, przybliżać zdolność generalizacji modelu, a my mamy do dyspozycji znaczniki czasowe, z kiedy pochodzą dane oceny. Można by więc użyć **podziału czasowego (time split)**, czyli wyodrębnić najnowsze oceny do zbioru testowego, a konkretnie najnowsze oceny per użytkownik. Stanowi to bardzo dobrą symulację tego, jak w praktyce działa system.\n",
        "\n",
        "Powyższe podejścia mają jednak pewne ryzyko - może się zdarzyć, że tak wylosujemy zbiór testowy, że dla jakiegoś użytkownika 90% ocen jest w zbiorze testowym, więc spowodujemy u niego przypadkiem problem zimnego startu. Analogicznie może być przy podziale czasowym, kiedy jakiś nowy użytkownik był aktywny tylko niedawno i być może nawet wszystkie jego predykcje trafiłyby do zbioru testowego. Dlatego można stosować **podział per użytkownik**, wyodrębniając np. losowe 10% ocen każdego użytkownika jako zbiór testowy.\n",
        "\n",
        "Jak widać, jest tu nieco ciężej niż przy zwykłej klasyfikacji czy regresji. Dla uproszczenia wykorzystamy zwykły podział losowy. Implementacje innych metod można znaleźć np. w bibliotece LibRecommender.\n",
        "\n",
        "Surprise definiuje 2 ważne klasy: `Dataset` i `Trainset`. Ta pierwsza reprezentuje surowe dane, a druga wstępnie przetworzone dane do treningu lub testowania. Interfejs jest tutaj dość dziwny, ale w skrócie:\n",
        "- do zwykłych algorytmów idą `train_set` i `test_set`\n",
        "- do `GridSearchCV` idą `data_train` i `test_set`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cwldYyVKiKQC"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from surprise.dataset import Dataset, Reader\n",
        "\n",
        "reader = Reader(rating_scale=(df[\"rating\"].min(), df[\"rating\"].max()))\n",
        "dataset = Dataset.load_from_df(df[[\"user_id\", \"item_id\", \"rating\"]], reader=reader)\n",
        "\n",
        "ratings_train, ratings_test = train_test_split(\n",
        "    dataset.raw_ratings, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "data_train = deepcopy(dataset)\n",
        "data_train.raw_ratings = ratings_train\n",
        "\n",
        "train_set = data_train.build_full_trainset()\n",
        "test_set = data_train.construct_testset(ratings_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9V-Qp_MiKQC"
      },
      "source": [
        "Na początek zaimplementujemy model, który przewiduje po prostu wartość średnią dla każdego przedmiotu. Nie ma on żadnych hiperparametrów, więc nawet nie będziemy potrzebować zbioru walidacyjnego. Jest to bardzo dobry **model bazowy (baseline)** w systemach rekomendacyjnych.\n",
        "\n",
        "**Zadanie 3 (1 punkt)**\n",
        "\n",
        "Uzupełnij kod klasy `ItemAveragePredictor`, która przewiduje wartość średnią dla każdego przedmiotu. Może ci się tutaj przydać atrybut `ir` (item rating) klasy `Trainset` - [dokumentacja](https://surprise.readthedocs.io/en/stable/trainset.html), oraz [dokumentacja tworzenia własnych algorytmów](https://surprise.readthedocs.io/en/stable/building_custom_algo.html).\n",
        "\n",
        "Dobrym pomysłem będzie przechowywanie danych w postaci atrybutu będącego słownikiem w `.fit()`, żeby zapamiętać mapowanie `item_id` -> średnia ocena.\n",
        "\n",
        "Uwaga - zgodnie z konwencją ze Scikit-learn atrybuty, których wartości są obliczane (estymowane) na podstawie danych treningowych, są tworzone w metodzie `.fit()` i mają underscore `_` na końcu nazwy, np. `self.ratings_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8StybhfCiKQC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from surprise import AlgoBase, PredictionImpossible\n",
        "\n",
        "\n",
        "class ItemAveragePredictor(AlgoBase):\n",
        "    def __init__(self):\n",
        "        AlgoBase.__init__(self)\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        # mapping: item_id -> average rating\n",
        "        self.ratings_ = {}\n",
        "\n",
        "\n",
        "        # compute average rating for each item\n",
        "\n",
        "        return self\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible(\"User and/or item is unknown.\")\n",
        "\n",
        "        return self.ratings_[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x395KhgYiKQC"
      },
      "outputs": [],
      "source": [
        "algo = ItemAveragePredictor()\n",
        "algo.fit(train_set)\n",
        "pred_item_avg = algo.test(test_set)\n",
        "pred_item_avg[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyhMazomiKQC"
      },
      "source": [
        "Musimy teraz ocenić jakość naszego algorytmu. Jako że mamy tu problem regresji, to naturalnym wyborem są **RMSE (root mean squared error)** oraz **MAE (Mean Absolute Error)**. Pokażą nam one, jak bardzo średnio nasz model myli się w przewidywaniu ratingu.\n",
        "\n",
        "RMSE to po prostu pierwiastek błędu średniokwadratowego (MSE). Ma taką samą wadę przy ewaluacji jak MSE - zwraca zbyt dużą uwagę na obserwacje odstające (outliers). Dzięki pierwiastkowaniu ma tę samą jednostkę, co oryginalne dane.\n",
        "$$\\large\n",
        "RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\left( y_i - \\hat{y}_i \\right)^2}\n",
        "$$\n",
        "\n",
        "MAE to średnie odchylenie predykcji od wartości prawdziwej. Dzięki zastosowaniu wartości bezwzględnej zamiast kwadratu jest miarą bardziej odporną na outliery i dlatego często wykorzystywaną przy ewaluacji. Ma naturalnie tę samą jednostkę, co mierzona wartość.\n",
        "$$\\large\n",
        "MAE = \\frac{1}{N} \\sum_{i=1}^N \\left| y_i - \\hat{y}_i \\right|\n",
        "$$\n",
        "\n",
        "Ze względu na to, że Surprise nie zwraca zwykłego wektora Numpy'a, tylko obiekty `Prediction`, trzeba użyć metryk z tej biblioteki. Zwykle nie stanowi to problemu, a dodatkowo mamy też do dyspozycji wszystko, co implementuje biblioteka recmetrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IpNlSiCiKQD"
      },
      "outputs": [],
      "source": [
        "from surprise.accuracy import rmse, mae\n",
        "\n",
        "rmse(pred_item_avg, verbose=True)\n",
        "mae(pred_item_avg, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsc2naQ6iKQD"
      },
      "source": [
        "Wygląda na to, że nawet najprostszy model nie radzi sobie wcale tak źle. Ale są to tylko przewidywane wartości - zobaczmy faktyczne rekomendacje. W praktyce mamy ograniczone miejsce, np. mało kto popatrzy na więcej niż pierwsze 5-10 rekomendowanych filmów. W związku z tym nieważne nawet, co będzie dalej - liczy się dla nas **top k** predykcji.\n",
        "\n",
        "Zgromadzimy teraz faktyczne najlepsze oceny ze zbioru testowego dla każdego użytkownika, rekomendacje naszego systemu i zbierzemy je w jednen DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA9hyt7JiKQD"
      },
      "outputs": [],
      "source": [
        "from surprise import Prediction\n",
        "\n",
        "\n",
        "def get_user_recommendations(user_rec_items: pd.Series) -> list[int]:\n",
        "    return user_rec_items.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "\n",
        "def get_recommendations(predictions: list[Prediction]) -> pd.DataFrame:\n",
        "    df_pred = pd.DataFrame(predictions)\n",
        "    df_pred = df_pred.drop(columns=\"details\")\n",
        "    df_pred.columns = [\"user_id\", \"item_id\", \"actual\", \"prediction\"]\n",
        "\n",
        "    df = (\n",
        "        df_pred.groupby(\"user_id\", as_index=False)[\"item_id\"]\n",
        "        .agg({\"actual\": (lambda x: list(x))})\n",
        "        .set_index(\"user_id\")\n",
        "    )\n",
        "\n",
        "    df_pivot = df_pred.pivot_table(\n",
        "        index=\"user_id\", columns=\"item_id\", values=\"prediction\"\n",
        "    ).fillna(0)\n",
        "\n",
        "    df[\"recommendations\"] = [\n",
        "        get_user_recommendations(df_pivot.loc[user_id]) for user_id in df.index\n",
        "    ]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "rec_item_avg = get_recommendations(pred_item_avg)\n",
        "rec_item_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIh9895HiKQD"
      },
      "source": [
        "Teraz kiedy można porównać faktyczne predykcje, patrząc np. na top 5, to nie wygląda to już tak dobrze, jak trzeba wybrać konkretne filmy. Do mierzenia jakości wśród top k predykcji służą metryki:\n",
        "- mean average precision at k (MAP@k)\n",
        "- mean average recall at k (MAR@k)\n",
        "- Fraction of Concordant Pairs (FCP)\n",
        "- Normalized Discounted Cumulative Gain (NDCG)\n",
        "\n",
        "Są one używane w systemach rekomendacyjnych, ale też w wyszukiwarkach i niektórych problemach NLP. Dla MAP i MAR dokładny opis, krok po kroku, możesz znaleźć [tutaj](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html) i [tutaj](https://machinelearninginterview.com/topics/machine-learning/mapatk_evaluation_metric_for_ranking/). FCP doskonale opisuje [oryginalny artykuł](https://www.ijcai.org/Proceedings/13/Papers/449.pdf). [Tutaj](https://finisky.github.io/2019/04/24/ndcg/) krótki i treściwy artykuł o NDCG.\n",
        "\n",
        "### MAP@k\n",
        "\n",
        "Zdefiniujmy:\n",
        "- True Positive (TP) - przedmiot, który naprawdę jest w top k (*relevant*) i który nasz system zarekomendował w top k przedmiotów\n",
        "- False Positive (FP) - przedmiot, który nie jest w top k (*nonrelevant*), ale nasz model go zarekomendował w top k\n",
        "\n",
        "\"Precision at k\" to precyzja (precision), obliczona dla top k przedmiotów. Oznaczmy przez $r_k$ liczbę TP (*relevant items*) wśród top k przedmiotów.\n",
        "$$\\large\n",
        "P@k = \\frac{\\text{number of relevant items in top }k}{k} = \\frac{r_k}{k}\n",
        "$$\n",
        "\n",
        "\"Average P@k\" to po prostu P@k obliczone dla różnych $i=1,2,...,k$ i uśrednione. Taka agregacja bardzo penalizuje umieszczanie nieciekawych przedmiotów na wysokich miejscach, bo uwzględniamy tylko precyzję dla tych top k, gdzie prawidłowo zauważyliśmy TP.\n",
        "$$\\large\n",
        "AP@k = \\frac{1}{r_k} \\sum_{i=1}^{k} \\left( P@i \\text{ if i-th item is relevant} \\right)\n",
        "$$\n",
        "\n",
        "MAP@k to AP@k (average precision at k), uśrednione dla wszystkich $|U|$ użytkowników:\n",
        "$$\\large\n",
        "MAP@k = \\frac{1}{|U|} \\sum_{u=1}^{|U|}AP@k(u)\n",
        "$$\n",
        "\n",
        "Im niższe $k$, tym surowsi jesteśmy i tym niższe będą wyniki - nasz algorytm ma mniej miejsca na błąd. Typowo $k=5$ lub $k=10$. Zakres wartości MAP@k to $[0, 1]$.\n",
        "\n",
        "MAP@k przykłada bardzo dużą wagę do tego, żeby na pierwszych miejscach trafiły się jak najlepsze przedmioty. Jest zatem bardzo ważne, kiedy mamy mało miejsc do dyspozycji, np. przy rekomendacji filmów na głównej stronie (Netflix).\n",
        "\n",
        "### FCP\n",
        "\n",
        "FCP (Fraction of Concordant Pairs) jest rzadziej używaną, ale bardzo intuicyjną metryką. Ideą jest uogólnienie metryki AUROC (ROC AUC) na algorytmy rankujące, a więc systemy rekomendacyjne. Ma zakres wartości $[0, 1]$.\n",
        "\n",
        "Liczba zgodnych par (*concordant pairs*) $n_c^u$ dla użytkownika $u$ to liczba par przedmiotów, które zostały prawidłowo uporządkowane przez ranker. Innymi słowy, gdy mamy prawdziwy ranking ocen użytkownika oraz przewidywany, to jest to liczba par przedmiotów, które prawidłowo ułożyliśmy (lepszy przedmiot wyżej niż gorszy).\n",
        "$$\\large\n",
        "n_c(u) = |\\{ (i,j) | \\hat{r}_{ui} > \\hat{r}_{uj} \\text{ and } r_{ui} > r_{uj}\\}|\n",
        "$$\n",
        "\n",
        "Pary niezgodne (*discordant pairs*) liczy się podobnie:\n",
        "$$\\large\n",
        "n_d(u) = |\\{ (i,j) | \\hat{r}_{ui} > \\hat{r}_{uj} \\text{ and } r_{ui} \\leq r_{uj}\\}|\n",
        "$$\n",
        "\n",
        "Proporcja par zgodnych do wszystkich, zsumowana dla wszystkich użytkowników, to FCP:\n",
        "$$\\large\n",
        "FCP = \\frac{n_c}{n_c + n_c} = \\frac{\\sum_{i=1}^n n_c(u)}{\\sum_{i=1}^n n_c(u) + n_d(u)}\n",
        "$$\n",
        "\n",
        "Można także obliczyć FCP@k, ograniczając się do pierwszych k predykcji.\n",
        "\n",
        "Metryka FCP przykłada mniejszą wagę niż MAP@k do tego, żeby najlepsze przedmioty były jak najwyżej. Skupia się natomiast na tym, żeby lepsze przedmioty były powyżej gorszych. Działa więc lepiej dla rekomendacji dłuższych list, kiedy pierwsze pozycje nie są aż tak ważne, np. przy rekomendowaniu playlist muzyki (Spotify)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP4UmivWiKQD"
      },
      "source": [
        "**Zadanie 4 (1 punkt)**\n",
        "\n",
        "Uzupełnij kod funkcji `ap_k`, która oblicza AP@k dla pojedynczego użytkownika. Pamiętaj, aby ograniczyć się do najwyższych (pierwszych) `k` przedmiotów dla rekomendacji oraz predykcji. W przypadku, gdy model nie miał żadnej dobrej predykcji ($r_k = 0$), zwróć 0.\n",
        "\n",
        "Następnie oblicz i wypisz MAP@k oraz FCP (k=10) dla naszego modelu średniej przedmiotu. Wartości podaj w procentach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p62zHVVmiKQD"
      },
      "outputs": [],
      "source": [
        "def ap_k(y_true: list[int], y_pred: list[int], k: int) -> float:\n",
        "    # implement me!\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcwQjnQBiKQD"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "\n",
        "from surprise.accuracy import fcp\n",
        "\n",
        "\n",
        "def map_k(df: pd.DataFrame, k: int) -> float:\n",
        "    ap_k_values = []\n",
        "    for idx, row in df.iterrows():\n",
        "        actual, recommendations = row\n",
        "        ap_k_val = ap_k(actual, recommendations, k)\n",
        "        ap_k_values.append(ap_k_val)\n",
        "\n",
        "    return np.mean(ap_k_values)\n",
        "\n",
        "\n",
        "def fcp_k(predictions: list[Prediction], k: int) -> float:\n",
        "    top_k = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_k[uid].append((iid, est))\n",
        "\n",
        "    user_item_id_pairs = set()\n",
        "\n",
        "    for user_id, user_ratings in top_k.items():\n",
        "        user_ratings.sort(key=itemgetter(1), reverse=True)\n",
        "        for item_id, rating in user_ratings[:k]:\n",
        "            user_item_id_pairs.add((user_id, item_id))\n",
        "\n",
        "    predictions_top_k = [\n",
        "        pred for pred in predictions if (pred[0], pred[1]) in user_item_id_pairs\n",
        "    ]\n",
        "\n",
        "    return fcp(predictions_top_k, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YytzisiZiKQD"
      },
      "outputs": [],
      "source": [
        "map_k_item_avg = map_k(rec_item_avg, k=10)\n",
        "fcp_item_avg = fcp_k(pred_item_avg, k=10)\n",
        "\n",
        "print(f\"Item average model MAP@k (k=10): {100 * map_k_item_avg:.2f}%\")\n",
        "print(f\"Item average model FCP@k (k=10): {100 * fcp_item_avg:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOW_pHGDiKQD"
      },
      "source": [
        "Zobaczymy, że ten wynik da się jeszcze poprawić.\n",
        "\n",
        "Metryki MAP@k i MAR@k mają jednak pewną wadę - preferują sugerowanie popularnych treści przez model, bo można je łatwo umieścić wysoko w rekomendacji i łatwo podbić sobie precyzję. W ten sposób rekomendacje byłyby słabo personalizowane. Dlatego wykorzystuje się szereg innych metryk, głównie biorących pod uwagę różnorodność i personalizację rekomendacji, na przykład:\n",
        "- pokrycie (*coverage*) - procent przedmiotów ze zbioru, który nasz system w ogóle rekomenduje\n",
        "- nowość (*novelty*) - zdolność systemu do rekomendacji zaskakujących, nowych dla użytkownika przedmiotów\n",
        "- personalizacja (*personalization*) - miara różnicy między rekomendacjami dla poszczególnych użytkowników\n",
        "\n",
        "Możesz o nich poczytać więcej na stronie [recmetrics](https://github.com/statisticianinstilettos/recmetrics) oraz w [tym artykule](https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-YXCUZFiKQD"
      },
      "source": [
        "**Pomiar jakości systemów rekomendacyjnych - podsumowanie**\n",
        "\n",
        "1. Poza stosowaniem zwykłego podziału losowego train-test można też stosować podział czasowy lub per użytkownik.\n",
        "2. Jednym z najprostszych modeli i dobrym punktem odniesienia (baseline) jest przewidywanie średniej per przedmiot.\n",
        "3. Podstawowymi metrykami jakości są metryki dla regresji: RMSE i MAE, oraz rankowania: MAP@k, MAR@k, NDCG.\n",
        "4. Inne metryki, specyficzne dla rekomendacji, biorą pod uwagę jakość personalizowanych rekomendacji, np. pokrycie, nowość, personalizacja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVt3rUGiiKQD"
      },
      "source": [
        "## Model średniej bayesowskiej"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN02_FX2iKQE"
      },
      "source": [
        "Masz ochotę na dobrą pizzę i szukasz opinii na Google Maps. Masz do wyboru 2 lokale: jeden ze średnią 5.0 i drugi ze średnią 4.8. Zauważasz jednak, że pierwszy ma tylko 5 opinii, a drugi 200. Który wybierzesz? Są spore szanse, że ten drugi, bo mamy większą **pewność (confidence)** co do oceny takiego lokalu.\n",
        "\n",
        "Prosty model, taki jak średnia przedmiotu, ma ten sam problem, co powyżej. Sformalizowaniem idei \"chcę być pewny, że ocena przedmiotu jest wysoka\" jest model **średniej bayesowskiej (Bayesian average)**. Możliwych sformułowań bayesowskich jest dużo, ale ogólna idea jest zawsze taka, aby wziąć pod uwagę rozkład ocen przedmiotu oraz ich liczbę. Co ważne, to dalej są rekomendacje globalne - mamy jedną predykcję per przedmiot.\n",
        "\n",
        "Czemu średnia \"bayesowska\"? Przypomnijmy sobie twierdzenie Bayesa:\n",
        "$$\\large\n",
        "P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}\n",
        "$$\n",
        "\n",
        "W naszym wypadku:\n",
        "1. $X$ - zbiór danych, który jest stały.\n",
        "2. $Y$ - przewidywane wartości.\n",
        "3. $P(X)$ - prawdopodobieństwo zaobserwowania naszych danych, które co prawda ciężko jest zmierzyć, ale na szczęście w ML zwykle możemy zignorować mianownik, bo to tylko stała.\n",
        "4. $P(Y)$ - *prior* (*prior distribution*), czyli z góry założony rozkład prawdopodobieństw wartości, które przewidujemy. Często zaczynamy bez żadnej wiedzy, więc zakładamy rozkład jednostajny lub normalny.\n",
        "5. $P(X|Y)$ - *likelihood*, wiarygodność, czyli jak dobrze model odwzorowuje dotychczas zaobserwowane dane.\n",
        "6. $P(Y|X)$ - *posterior* (*posterior distribution*), czyli docelowy rozkład wartości przewidywanych, obliczony na podstawie danych.\n",
        "\n",
        "W kontekście systemów rekomendacyjnych:\n",
        "- $P(Y)$ (prior) to założony z góry rozkład ocen, typowo jednostajny, czyli jest taka sama szansa na każdą ocenę\n",
        "- $P(X|Y)$ (likelihood) to miara, jak dobrze nasz model odwzorowuje macierz ocen; jakbyśmy potraktowali go jako skrzynkę generującą oceny, to wiarygodność mierzy, jak bliskie są te generowane wartości wobec prawdziwych ze zbioru danych\n",
        "- $P(Y|X)$ (posterior) to rozkład przewidywanych ocen dla poszczególnych przedmiotów\n",
        "\n",
        "Jak widać, dostajemy rozkład w wyniku. Jak dostać konkretną predykcję, czyli np. liczbę gwiazdek? Używamy **maximum a posteriori (MAP)**, czyli bierzemy po prostu tę ocenę, dla której rozkład posterior ma największą wartość.\n",
        "\n",
        "Wykorzystamy podejście opisane krok po kroku [w tym artykule](https://fulmicoton.com/posts/bayesian_rating/) oraz [tym tutorialu](https://www.algolia.com/doc/guides/managing-results/must-do/custom-ranking/how-to/bayesian-average/), w którym przewidywana ocena dla $i$-tego przedmiotu (po przekształceniach) to:\n",
        "$$\\large\n",
        "r_i = \\frac{C \\cdot m + \\text{suma ocen dla przedmiotu } i}{C + \\text{liczba ocen}}\n",
        "$$\n",
        "\n",
        "gdzie:\n",
        "- $m$ - prior, globalna średnia ocen dla wszystkich przedmiotów\n",
        "- $C$ - confidence, liczba ocen dla przedmiotu\n",
        "- liczba ocen\n",
        "\n",
        "Dodatkowe źródła:\n",
        "- [artykuł o twierdzeniu Bayesa](https://towardsdatascience.com/understand-bayes-rule-likelihood-prior-and-posterior-34eae0f378c5)\n",
        "- [proste i przyjazne sformułowanie średniej bayesowskiej](https://arpitbhayani.me/blogs/bayesian-average)\n",
        "- [bardziej wyrafinowane podejście oparte o dolną granicę błędu](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html)\n",
        "- [bardzo wyrafinowane podejście oparte o dolną granicę błędu - dla odważnych](https://www.evanmiller.org/ranking-items-with-star-ratings.html)\n",
        "- [średnia bayesowska dla danych zmiennych w czasie](https://www.evanmiller.org/bayesian-average-ratings.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF9qShEOiKQH"
      },
      "source": [
        "**Zadanie 5 (1 punkt)**\n",
        "\n",
        "Uzupełnij kod klasy `BayesianAveragePredictor`. W metodzie `.fit()` musisz obliczyć parametry:\n",
        "- sumę ocen dla każdego przedmiotu\n",
        "- liczbę ocen dla każdego przedmiotu\n",
        "- globalną pewność (confidence, $C$)\n",
        "\n",
        "Pewność oblicz jako dolny kwartyl (25 percentyl) rozkładu liczby ocen przedmiotów, zgodnie z [tym tutorialem](https://www.algolia.com/doc/guides/managing-results/must-do/custom-ranking/how-to/bayesian-average/#how-to-calculate-the-bayesian-average). Przyda ci się funkcja `np.quantile()`.\n",
        "\n",
        "Sugerowane jest używanie słowników w `.fit()`, żeby mapować `item_id` na odpowiednią wartość.\n",
        "\n",
        "W metodzie `.estimate()` musisz zastosować obliczone parametry we wzorze podanym powyżej.\n",
        "\n",
        "Dokonaj predykcji i oblicz metryki za pomocą podanej funkcji. Skomentuj wynik w porównaniu do przewidywania średniej przedmiotu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55PYh8E6iKQH"
      },
      "outputs": [],
      "source": [
        "class BayesianAveragePredictor(AlgoBase):\n",
        "    def __init__(self):\n",
        "        AlgoBase.__init__(self)\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        self.global_avg_ = trainset.global_mean\n",
        "\n",
        "        # mapping: item_id -> sum of ratings\n",
        "\n",
        "        # mapping: item_id -> number of ratings\n",
        "\n",
        "        # compute rating sum for each item\n",
        "\n",
        "        # confidence (C)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible(\"User and/or item is unknown.\")\n",
        "\n",
        "        C = ...\n",
        "        m = ...\n",
        "        item_sum = ...\n",
        "        item_count = ...\n",
        "\n",
        "        score = ...\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPvROiSfiKQH"
      },
      "outputs": [],
      "source": [
        "def print_metrics(\n",
        "    predictions: list[Prediction], recommendations: pd.DataFrame, k: int = 10\n",
        ") -> None:\n",
        "    rmse(predictions, verbose=True)\n",
        "    mae(predictions, verbose=True)\n",
        "    map_k_value = map_k(recommendations, k=k)\n",
        "    fcp_k_value = fcp_k(predictions, k=k)\n",
        "\n",
        "    print(f\"MAP@k ({k=}): {100 * map_k_value:.2f}%\")\n",
        "    print(f\"FCP@k ({k=}): {100 * fcp_k_value:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfmz-kvSiKQH"
      },
      "outputs": [],
      "source": [
        "algo = BayesianAveragePredictor()\n",
        "algo.fit(train_set)\n",
        "pred_bayes_avg = algo.test(test_set)\n",
        "rec_bayes_avg = get_recommendations(pred_bayes_avg)\n",
        "\n",
        "# calculate and print metrics\n",
        "\n",
        "print_metrics(pred_bayes_avg, rec_bayes_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atjHt1ciKQI"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzkfpF7hiKQI"
      },
      "source": [
        "## Metody oparte o sąsiedztwo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFlZjHJKiKQI"
      },
      "source": [
        "Mając solidne punkty odniesienia w postaci rekomendacji globalnych, możemy przejść do rekomendacji personalizowanych. W metodach **opartych o sąsiedztwo (neighborhood-based)** znajdujemy podobnych użytkowników do nas, albo przedmioty podobne do tych które lubiliśmy, i na podstawie tego dokonujemy rekomendacji.\n",
        "\n",
        "Podejście to jest używane także w innych obszarach uczenia maszynowego, np. w algorytmie k najbliższych sąsiadów (*k nearest neighbors*, kNN), SMOTE, albo w identyfikacji osób (znajdujemy 1 najbliższego sąsiada dla embeddingu twarzy). Wymaga ono odpowiedniej metryki, która zmierzy odległość między wektorami, znajdując k najbliższych sąsiadów, z których następnie wyciągamy informacje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janAazULiKQI"
      },
      "source": [
        "### User-based neighborhood-based CF\n",
        "\n",
        "Idea podejścia **user-based** jest bardzo prosta - znajdźmy użytkowników podobnych do nas, którzy oceniali przedmioty, których my jeszcze nie widzieliśmy, i zasugerujmy to, co potencjalnie najbardziej będzie się nam podobać. Realizuje podejście \"użytkownicy podobni do ciebie oglądali także...\".\n",
        "\n",
        "Algorytm user-based collaborative filtering działa następująco:\n",
        "1. Dla każdego użytkownika znajdź k najbliższych sąsiadów\n",
        "2. Predykcja dla przedmiotu to średnia ocena sąsiadów dla tego przedmiotu, którzy ocenili dany przedmiot\n",
        "3. Zarekomenduj te przedmioty, które mają najwyższą przewidywaną ocenę\n",
        "\n",
        "Co ważne, przy obliczaniu najbliższych użytkowników bierzemy tylko te przedmioty, które obaj ocenili. Przykładowo, jeżeli użytkownik $u_1$ ocenił przedmioty $[1, 2, 3]$, a użytkownik $u_2$ ocenił przedmioty $[2, 3, 4]$, to na potrzeby obliczania ich podobieństwa bierzemy pod uwagę tylko $[2, 3]$. Przy obliczaniu predykcji dla $i$-tego przedmiotu także bierzemy pod uwagę tylko tych najbliższych sąsiadów, którzy wystawili mu ocenę.\n",
        "\n",
        "Predykcja dla użytkownika $u$ i przedmiotu $i$ to:\n",
        "$$\\large\n",
        "\\hat{r}_{ui} = \\frac{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v) * r_{vi}}{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v)}\n",
        "$$\n",
        "gdzie:\n",
        "- $N_i^k(u)$ - $k$ najbliższych sąsiadów dla użytkownika $u$, którzy ocenili przedmiot $i$\n",
        "- $r_{vi}$ - ocena przedmiotu $i$ przez użytkownika $v$\n",
        "- $\\text{sim}(u, v)$ - podobieństwo użytkowników $u$ i $v$ według metryki $\\text{sim}$\n",
        "\n",
        "Co ważne, tutaj metryka jest podobieństwem, tzn. większa wartość = bardziej podobni użytkownicy. Typowo używa się **korelacji Pearsona (Pearson correlation)**, która przyjmuje wartości z zakresu $[-1, 1]$. Dzięki temu wiemy, którzy użytkownicy są bardzo podobni (blisko 1), którzy mają wręcz przeciwny gust do naszego (blisko -1), a którzy są w ogóle inni od nas (blisko 0). Niektóre implementacje (np. Surprise) biorą pod uwagę tylko sąsiadów o nieujemnej korelacji, a inne wykorzystują tę informację z ujemną wagą."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krd636tHiKQI"
      },
      "outputs": [],
      "source": [
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "\n",
        "knn_basic = KNNBasic(sim_options={\"name\": \"pearson\"})\n",
        "knn_basic.fit(train_set)\n",
        "pred_knn_basic = knn_basic.test(test_set)\n",
        "rec_knn_basic = get_recommendations(pred_knn_basic)\n",
        "\n",
        "print_metrics(pred_knn_basic, rec_knn_basic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwJXCtPeiKQI"
      },
      "source": [
        "Wynik nie jest może idealny, ale nie przeprowadziliśmy jeszcze żadnego tuningu hiperparametrów.\n",
        "\n",
        "Najważniejszym hiperparametrem jest **liczba sąsiadów `k`**. Trzeba wziąć pod uwagę, że nie wszystkie przedmioty będą się pokrywać między użytkownikami, więc typowo bierze się ich dość dużo. Jeżeli dana implementacja uwzględnia tylko nieujemne korelacje, to jeszcze więcej sąsiadów może odpaść, więc trzeba wziąć większą wartość. Jest to więc de facto maksymalna liczba sąsiadów do uwzględnienia. Im większa wartość, tym mocniejsza regularyzacja, bo uśredniamy więcej użytkowników. Przede wszystkim należy jednak wziąć pod uwagę wielkość naszego zbioru, szczególnie liczbę użytkowników oraz gęstość.\n",
        "\n",
        "Drugim hiperparametrem jest **minimalna liczba sąsiadów `min_k`**. Jeżeli spośród `k` najbliższych sąsiadów mniej niż `min_k` oceniło dany przedmiot, to mamy zimny start. Zwykle wykorzystuje się wtedy algorytm globalny, np. przewidując globalną średnią. Jak widać, system rekomendacyjny składa się w środku z bardzo wielu systemów rekomendacyjnych :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNA3bPjiKQI"
      },
      "source": [
        "**Zadanie 6 (1 punkt)**\n",
        "\n",
        "Przeprowadź tuning hiperparametrów, używając 10-krotnej walidacji skrośnej i optymalizując MAE. Jako że nasz zbiór jest dość mały, to sprawdzimy zakres:\n",
        "```\n",
        "param_grid = {\n",
        "    \"k\": list(range(10, 51, 10)),\n",
        "    \"min_k\": list(range(1, 4)),\n",
        "    \"sim_options\": {\"name\": [\"pearson\"]},\n",
        "    \"random_state\": [0],\n",
        "    \"verbose\": [False]\n",
        "}\n",
        "```\n",
        "\n",
        "Jako że interesują nas przede wszystkim same rekomendacje, optymalizuj metrykę FCP. Wypisz znalezione najlepsze hiperparametry oraz metryki na zbiorze testowym dla najlepszego modelu.\n",
        "\n",
        "Wskazówki:\n",
        "- `GridSearchCV` z biblioteki Surprise\n",
        "- argument `refit` ma domyślną wartość `False`, inaczej niż w Scikit-learn'ie\n",
        "- argument `n_jobs`\n",
        "- niestety `random_state` trzeba przekazać jako hiperparametr, API Surprise jest tutaj niezbyt dobrze zrobione ([Github issue](https://github.com/NicolasHug/Surprise/issues/212))\n",
        "- analogicznie do powyższego trzeba przekazać dość dziwnie `sim_options`\n",
        "- analogicznie do powyższego działa przekazywanie `verbose` (żeby uniknąć zalewu tekstu)\n",
        "\n",
        "Skomentuj wyniki i zmiany w poszczególnych metrykach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNnZsNOziKQI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mSmLXZziKQI"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0TW817KiKQI"
      },
      "source": [
        "Ten algorytm nie bierze jednak psychologicznych różnic między użytkownikami. Niektórzy użytkownicy będą średnio zawyżać oceny, bo film to dla nich luźna rozrywka, a poważni koneserzy mogą dawać filmom średnio dość niskie oceny. Taka tendencja to **user bias**, ale na szczęście można go policzyć - to po prostu średnia ocena wystawiana przez użytkownika, a więc średnia z każdego wiersza w macierzy ocen.\n",
        "\n",
        "Jeżeli od każdego wiersza odejmiemy jego średnią, to dostaniemy **ratings deviations**, czyli nie mamy już w macierzy samych ocen, tylko jak bardzo ocena danego przedmiotu przez użytkownika różni się od jego średniej predykcji. Taka operacja to **centrowanie (centering)**. Na takich wartościach można też zwyczajnie liczyć najbliższych sąsiadów, a korelacja Pearsona dalej działa dla takich danych. Żeby dokonać predykcji, przewidujemy odchylenie dla przedmiotu, a następnie dodajemy je dla średniej danego użytkownika.\n",
        "\n",
        "Mamy zatem:\n",
        "$$\\large\n",
        "\\hat{r}_{ui} = \\mu_i + \\frac{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v) * (r_{vi} - \\mu_v)}{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAeI0G4kiKQI"
      },
      "source": [
        "**Zadanie 7 (0.5 punktu)**\n",
        "\n",
        "Analogicznie do poprzedniego zadania wytrenuj, zoptymalizuj i sprawdź na zbiorze treningowym user-based CF z centrowaniem (`KNNWithMeans`). Wypisz także optymalny zestaw hiperparametrów dla obu algorytmów.\n",
        "\n",
        "Skomentuj uzyskane hiperparametry i wyniki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghgq43QFiKQI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZdU-RqfiKQJ"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecFqWAbLiKQJ"
      },
      "source": [
        "### Item-based neighborhood-based CF\n",
        "\n",
        "Idea podejścia **item-based** jest bardzo podobna do user-based, ale znajdujemy podobne przedmioty, a nie użytkowników. Operujemy zatem na kolumnach macierzy ocen. Realizuje to podejście \"mogą cię jeszcze zainteresować przedmioty...\" oraz \"skoro oglądałeś X, to mogą spodobać ci się...\".\n",
        "\n",
        "Predykcja dla użytkownika $u$ i przedmiotu $i$ to:\n",
        "$$\\large\n",
        "\\hat{r}_{ui} = \\frac{\\sum_{j \\in N_u^k(i)} \\text{sim}(u, v) * r_{uj}}{\\sum_{j \\in N_u^k(i)} \\text{sim}(u, v)}\n",
        "$$\n",
        "\n",
        "Podobieństwo przedmiotów liczymy tutaj według kolumn macierzy, a metryką jest zwykle **podobieństwo cosinusowe (cosine similarity)**. Wykorzystuje się także centrowanie, eliminując **item bias** - przykładowo, \"Titanic\" będzie miał zwykle zawyżone oceny, bo każdy słyszał, że to znany i dobry film, więc podświadomie zawyżymy mu ocenę. Metrykę po centralizacji nazywa się czasem *adjusted cosine similarity*.\n",
        "\n",
        "Podejście item-based zazwyczaj daje większą dokładność niż used-based, tzn. niższe RMSE i MAE. Skutkuje to jednak niższym pokryciem czy nowością. Takie podejście potrafi być też bardziej czułe na zimny start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU3Xc-FRiKQJ"
      },
      "source": [
        "**Zadanie 8 (0.5 punktu)**\n",
        "\n",
        "Zaimplementuj podejście item-based z metryką cosinusową w wariantach:\n",
        "- bez normalizacji\n",
        "- z centrowaniem (adjusted cosine)\n",
        "\n",
        "Analogicznie do poprzedniego ćwiczenia zastosuj optymalizację hiperparametrów, podaj najlepszy zestaw oraz wypisz metryki na zbiorze testowym.\n",
        "\n",
        "Żeby zamienić algorytm user-based na item-based, oraz zmienić metrykę, przyda ci się [ten tutorial](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv).\n",
        "\n",
        "Skomentuj, jaką uzyskano różnicę względem user-based i które rozwiązanie twoim zdaniem jest lepsze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmPM0HpwiKQJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IumLca4JiKQJ"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgyu_8efiKQJ"
      },
      "source": [
        "### Metody oparte o sąsiedztwo - podsumowanie\n",
        "\n",
        "Podsumowanie:\n",
        "1. Możemy wyróżnić dwa sposoby liczenia sąsiadów: user-based (inni użytkownicy, wiersze macierzy) oraz item-based (inne przedmioty, kolumny macierzy).\n",
        "2. Podejście user-based wykorzystuje zwykle korelację Pearsona, a item-based podobieństw cosinusowe.\n",
        "3. Użytkownicy oraz przedmioty mają naturalny bias (user bias, item bias), który można wyeliminować, stosując normalizację: centrowanie lub standaryzację.\n",
        "\n",
        "Zalety:\n",
        "1. Prostota\n",
        "2. Interpretowalność, szczególnie dla item-based\n",
        "3. Stosunkowo niewielka czułość na dobór hiperparametrów\n",
        "4. Można idealnie uwspółbieżnić trening oraz predykcję (embarassingly parallel)\n",
        "\n",
        "Wady:\n",
        "1. Dość trudna implementacja, trzeba wybierać wspólne przedmioty\n",
        "2. Trening jest niezbyt skalowalny dla bardzo dużych danych\n",
        "3. Czułe na zimny start\n",
        "4. Bardzo niewiele implementacji wspiera dodawanie nowych użytkowników/przedmiotów - trzeba przetrenowywać regularnie cały model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6kZ0AJwiKQJ"
      },
      "source": [
        "## Metody oparte o rozkład macierzy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpsW5Uq8iKQJ"
      },
      "source": [
        "Podejście najbliższych sąsiadów definiuje \"sąsiedztwo\" bardzo explicite - wymaga, by użytkownicy ocenili dokładnie te same filmy, aby w ogóle sprawdzać, czy są podobni. Nie wykorzystuje to niejawnych podobieństw między przedmiotami i filmami. Przykładowo, jeżeli jeden lubi filmy \"Szeregowiec Ryan\", \"Dunkierka\" i \"Wróg u bram\", a drugi lubi filmy \"Czas apokalipsy\" i \"Jak rozpętałem drugą wojnę światową\", to są do siebie bardzo podobni, a jednak podejście user-based nawet nie będzie w stanie tego sprawdzić. Item-based mogłoby tu nieco pomóc, ale tam mogą się zdarzyć analogiczne sytuacje.\n",
        "\n",
        "Podejście oparte o rozkład macierzy, spopularyzowane w ramach konkursu Netflix Prize 2007 przez Simona Funka ([wywiad](https://www.kdd.org/exploration_files/simon-funk-explorations.pdf), [jego blog](https://sifter.org/simon/journal/20061211.html)), rozwiązuje właśnie ten problem. Stanowi kamień milowy w systemach rekomendacyjnych, gdyż jest daje bardzo dobre wyniki, doskonale uwspółbieżnia się i rozprasza na wiele maszyn, a do tego jest naprawdę proste. Szczegółowy i bardzo przystępny opis tego podejścia można znaleźć w artykule [\"Matrix factorization techniques for recommender systems\" Y. Koren, R. Bell, C. Volinsky](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf).\n",
        "\n",
        "Ideą jest, aby dokonać **rozkładu macierzy (matrix decomposition)** na macierzy ocen $R$, przybliżając ją jako iloczyn dwóch macierzy $W$ (user matrix) i $U$ (item matrix), które reprezentują użytkowników i przedmioty po dekompozycji:\n",
        "$$\\large\n",
        "\\hat{R} = WU^T\n",
        "$$\n",
        "\n",
        "![matrix_decomposition.png](attachment:matrix_decomposition.png)\n",
        "\n",
        "Załóżmy, że mamy $N$ użytkowników i $M$ przedmiotów. Kształty to zatem:\n",
        "$$\\large\n",
        "\\hat{R}_{N \\times M} = W_{N \\times K} U_{K \\times N}^T\n",
        "$$\n",
        "\n",
        "Pojawił nam się nowy wymiar $K$ - każdy użytkownik to teraz wektor z macierzy $W$ o długości $K$, a każdy przedmiot to wektor z macierzy $U$ o długości $K$. Jest to **ukryta wymiarowość (latent dimensionality)**, stanowiąca hiperparametr, analogiczny np. do rozmiaru warstw sieci neuronowej. Nie są to interpretowalne cechy, ale można zauważyć przy dobrym modelu, że odwzorowują pewne ogólne tematy w danych. Przykładowo, dla filmów (przedmiotów) mogą oznaczać, jak dużo jest poszczególnych tematów w filmie, np. \"romans\", \"komedia\", \"akcja\". Dla użytkowników mogą oznaczać, w jak dużym stopniu użytkownik interesuje się danym tematem. Typowe wartości $K$ to około kilkadziesiąt-kilkaset. Ze względu na wykorzystanie latent dimension takie modele nazywa się też **latent factor models**.\n",
        "\n",
        "Predykcji w takim modelu dokonuje się przez iloczyn skalarny wektora użytkownika (wiersz $W$) z wektorem przedmiotu (kolumna $U$):\n",
        "$$\\large\n",
        "r_{ui} = w_u^Tu_i\n",
        "$$\n",
        "\n",
        "Dzięki takiemu sformułowaniu, jeśli zainteresowanie użytkownika tematem $k$ będzie duże, a film będzie zawierał dużo tematu $k$, to ich mnożenie da dużą wartość, a zatem dużą predykcję.\n",
        "\n",
        "Algorytm ten nazywa się czasem niepoprawnie SVD, bo takiej nazwy użył Simon Funk do opisu swojego algorytmu (jego wersja ma trochę ulepszeń; będziemy ją nazywać FunkSVD). Co ważne, nie wykorzystujemy tutaj algorytmu SVD, bo nie potrzebujemy całego jego aparatu matematycznego. Zamiast tego ten algorytm to po prostu **matrix factorization (MF)**, tudzież **Probabilistic Matrix Factorization (PMF)** ([oryginalny artykuł PMF](https://proceedings.neurips.cc/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf) dowodzi, że to sformułowanie jest poprawne probabilistycznie). Trenuje się go także bez SVD, zamiast tego wykorzystując spadek wzdłuż gradientu lub algorytm **Alternating Least Squares (ALS)**. Ciężko powiedzieć, które podejście jest lepsze, patrz np. [ta dyskusja](https://stats.stackexchange.com/questions/201279/comparison-of-sgd-and-als-in-collaborative-filtering), [ten paper](http://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf).Oba podejścia bardzo dobrze opisuje [ten artykuł](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/), który rozwija też bardziej formalnie, czemu ekstrakcja \"tematów\" działa (spoiler: MF dokonuje implicite klasteryzacji).\n",
        "\n",
        "Niezależnie od podejścia, minimalizuje się funkcję kosztu, czyli różnicę między naszym przybliżeniem $\\hat{R} = WU^T$ a prawdziwą macierzą $R$. Zwykle wykorzystuje się tutaj błąd średniokwadratowy, w zapisie macierzowym $||\\hat{R} - R||_2^2$. Zapisując to ręcznie:\n",
        "$$\\large\n",
        "L = \\sum_{u, i \\in \\Omega} \\left( r_{ui} - \\hat{r}_{ui} \\right)^2\n",
        "$$\n",
        "gdzie $\\Omega$ to zbiór wszystkich wypełnionych komórek w macierzy ocen.\n",
        "\n",
        "Jako że mamy dwie macierze do nauczenia, $W$ oraz $U$, to mamy pochodną po wektorach $w$ oraz po wektorach $u$. Po przekształceniach dostajemy:\n",
        "$$\\large\n",
        "w_i = \\left( \\sum_{j \\in \\Psi_i} u_ju_j^T \\right)^{-1} \\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
        "$$\n",
        "$$\\large\n",
        "u_j = \\left( \\sum_{i \\in \\Omega_j} w_iw_i^T \\right)^{-1} \\sum_{i \\in \\Omega_j} r_{ij}w_i\n",
        "$$\n",
        "gdzie:\n",
        "- $\\Psi_i$ oznacza zbiór przedmiotów, które ocenił użytkownik $i$\n",
        "- $\\Omega_j$ oznacza zbiór użytkowników, którzy ocenili przedmiot $j$\n",
        "\n",
        "Można zauważyć, że w obu przypadkach jest to zwyczajny nadokreślony (*overdetermined*) układ równań postaci $x=A^{-1}b$. Taki problem rozwiązuje się metodą najmniejszych kwadratów (*ordinary least squares*), stąd część nazwy metody. Oczywiście do rozwiązania problemu OLS można użyć SVD.\n",
        "\n",
        "Przybliżymy teraz krótko metodę ALS, bo SGD znamy już z sieci neuronowych. Można zauważyć w powyższych wzorach, że dla $W$ pochodna zależy od wartości w $U$, a dla $U$ od wartości w $W$ - wydaje się, że sytuacja patowa. Rozwiązaniem jest po prostu przyjąć losowy punkt wyjścia, a potem raz rozwiązywać $W$ za pomocą $U$, a raz na odwrót.\n",
        "\n",
        "Pełny algorytm ALS:\n",
        "1. Zainicjalizuj losowo macierze $W$ i $U$ niewielkimi wartościami z rozkładu normalnego\n",
        "2. Powtarzaj przez T kroków:\n",
        "  1. Zaktualizuj $U$ według wzoru, rozwiązując układ równań; $W$ jest stałe\n",
        "  2. Zaktualizuj $W$ według wzoru, rozwiązując układ równań; $U$ jest stałe\n",
        "\n",
        "Co ważne, zbieżność i ALS, i spadku wzdłuż gradientu jest gwarantowana, ale do minimum lokalnego. Zwykle nie stanowi to jednak problemu, a w razie czego zawsze można wytrenować wiele modeli na różnych `random_state` i wybrać najlepszy. Liczba epok treningowych stanowi dość prosty hiperparametr - im więcej, tym dokładniejsi po prostu będziemy, więc możemy bardziej overfitować (analogicznie do sieci neuronowych). Zazwyczaj w przypadku ALS wystarcza niewielka liczba, kilka-kilkanaście."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqCuEfY4iKQJ"
      },
      "source": [
        "Surprise implementuje wersję z SGD. Wersję z ALS implementuje np. Apache Spark. Wersję z SGD można też łatwo zaimplementować w dowolnym frameworku do sieci neuronowych, np. PyTorch czy TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBcUggPAiKQJ"
      },
      "outputs": [],
      "source": [
        "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
        "\n",
        "# regular MF - no user/item bias, no regularization\n",
        "mf = SVD(biased=False, reg_all=0, random_state=0)\n",
        "mf.fit(train_set)\n",
        "pred_mf = mf.test(test_set)\n",
        "rec_mf = get_recommendations(pred_mf)\n",
        "\n",
        "print_metrics(pred_mf, rec_mf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prtii_xriKQJ"
      },
      "source": [
        "Dostaliśmy całkiem dobry wynik bez żadnego tuningu, a czeka nas jeszcze trochę ulepszeń, bo powyższy algorytm to jeszcze nie słynny FunkSVD.\n",
        "\n",
        "Skoro user bias i item bias pomagały w metodach opartych o sąsiedztwo, to dodajmy je też tutaj, czemu nie. Najpierw możemy odjąć od wszystkiego globalną średnią $\\mu$, żeby wycentrować całą macierz. Potem odejmujemy od każdego wiersza user bias $b_u$, a na koniec od każdej kolumny item bias $b_i$. Predykcja to zatem:\n",
        "$$\\large\n",
        "\\hat{r}_{ui} = \\mu + b_u + b_i + w_u^Tu_i\n",
        "$$\n",
        "\n",
        "Drugie ulepszenie to dodanie regularyzacji do naszej funkcji kosztu. W końcu nie możemy się zbyt bardzo dostosować do zbioru treningowego, nasz algorytm ma generalizować się dla przyszłych rekomendacji. Co ważne, mamy tutaj aż 4 możliwe źródła przeuczenia:\n",
        "- $b_u$ - zbytnie dostosowanie do dotychczasowych odchyleń użytkowników\n",
        "- $b_i$ - analogicznie, ale dla przedmiotów\n",
        "- $w_i$ - jest to wektor wag, więc duże wagi oznaczają overfitting, jak np. w regresji liniowej\n",
        "- $u_i$ - analogicznie, ale dla drugiej macierzy\n",
        "\n",
        "Można by użyć 4 osobnych współczynników regularyzacji, ale optymalizacja takiej siatki hiperparametrów jest raczej mało wykonalna. Można więc użyć jednego hiperparametru na moc regularyzacji L2 $\\lambda$, włączając do niego wszystkie parametry. Daje to funkcję kosztu:\n",
        "$$\\large\n",
        "L = \\sum_{u, i \\in \\Omega} \\left( r_{ui} - \\hat{r}_{ui} \\right)^2 + \\lambda \\left( ||W||_2^2 + ||U||_2^2 + ||b_u||_2^2 + ||b_i||_2^2 \\right)\n",
        "$$\n",
        "\n",
        "Pomijając dalsze wyprowadzenie, nic nie zmienia to w gruncie rzeczy w algorytmie ALS, dalej możemy użyć zwykłego OLS, zmienią się tylko trochę wartości w macierzach. Niewiele zmienia się też, gdy używamy spadku wzdłuż gradientu - dodajemy tylko regularyzację do funkcji kosztu.\n",
        "\n",
        "Powyższe sformułowanie to już pełny algorytm FunkSVD. Zobaczmy, jak sobie poradzi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf0QWQ3hiKQJ"
      },
      "outputs": [],
      "source": [
        "funk_svd = SVD(biased=True, random_state=0)\n",
        "funk_svd.fit(train_set)\n",
        "pred_funk_svd = funk_svd.test(test_set)\n",
        "rec_funk_svd = get_recommendations(pred_funk_svd)\n",
        "\n",
        "print_metrics(pred_funk_svd, rec_funk_svd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qb6GwziKQJ"
      },
      "source": [
        "Wygląda to na bardzo dobry wynik, a nie dokonaliśmy jeszcze żadnego tuningu hiperparametrów.\n",
        "\n",
        "**Zadanie 9 (0.5 punktu)**\n",
        "\n",
        "Zaimplementuj tuning hiperparametrów dla algorytmu FunkSVD, sprawdzając siatkę hiperparametrów:\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_factors\": list(range(50, 151, 10)),\n",
        "    \"lr_all\": [0.001, 0.003, 0.005, 0.007, 0.01],\n",
        "    \"reg_all\": [0.01, 0.02, 0.03]\n",
        "}\n",
        "```\n",
        "\n",
        "Pamiętaj, aby przekazać stałe `random_state`! Przyda się też `n_jobs`. Jeżeli na twoim sprzęcie będzie się to liczyć o wiele za długo, to możesz zmniejszyć zakres `n_factors` do 80-121.\n",
        "\n",
        "Skomentuj wyniki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OTI5YZZiKQK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td98kmgyiKQK"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j164EIbniKQK"
      },
      "source": [
        "### Metody oparte o rozkład macierzy - podsumowanie\n",
        "\n",
        "Podsumowanie:\n",
        "1. Macierz ocen można zdekomponować do iloczynu macierzy użytkowników $W$ oraz macierzy przedmiotów $U$.\n",
        "2. W tym podejściu wprowadzamy dodatkowy ukryty wymiar (latent dimension) wielkości $K$, który reprezentuje tematy ukryte w naszych danych.\n",
        "3. Do obliczania macierzy minimalizuje się błąd przybliżenia macierzy ocen przez nasze macierze $W$ i $U$. Służy do tego albo spadek wzdłuż gradientu, albo, bardziej typowo, algorytm Alternating Least Squares (ALS).\n",
        "\n",
        "Zalety:\n",
        "1. Bardzo dobre wyniki\n",
        "2. Szybkość i skalowalność\n",
        "3. Możliwość przyspieszenia obliczeń z pomocą GPU\n",
        "4. Działa dość dobrze w przypadku zimnego startu.\n",
        "\n",
        "Wady:\n",
        "1. Dość dużo hiperparametrów, przynajmniej w przypadku użycia spadku wzdłuż gradientu\n",
        "2. Brak optymalnego wyniku, trzeba by wypróbować różne losowe punkty startowe\n",
        "3. Niska interpretowalność\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhD6AS_PiKQK"
      },
      "source": [
        "## Projektowanie systemów rekomendacyjnych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-38aARliKQK"
      },
      "source": [
        "Podczas projektowania faktycznych systemów rekomendacyjnych trzeba rozważyć wiele nieoczywistych kwestii. O wielu z nich mówiliśmy już podczas tego laboratorium. Teraz można zmierzyć się z paroma takimi kwestiami."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_g2EG1GiKQK"
      },
      "source": [
        "**Zadanie 10 (1 punkt)**\n",
        "\n",
        "Odpowiedz na poniższe pytania. Każde pytanie jest warte 0.25 punktu.\n",
        "\n",
        "1. Załóżmy, że mamy system rekomendujący reklamy użytkownikom. Reklamy zmieniają się regularnie i dość często. Czy algorytmy collaborative filtering będą tu dobrym wyborem, a jeśli tak, to jakie? Jeżeli nie, to co będzie stanowić tu główny problem?\n",
        "\n",
        "2. Wymyśl i krótko opisz architekturę przykładowego systemu rekomendacyjnego, sugerującego gry w sklepie Steam. Opisz, jakich algorytmów użyłbyś w konkretnych przypadkach i czemu. Uwzględnij, że może wystąpić zimny start w przypadku nowych użytkowników oraz gier, oraz że mamy ogromną przestrzeń możliwych gier - każdy użytkownik zagra tylko w niewielki ułamek.\n",
        "\n",
        "3. Pracujesz w firmie obsługującej platformy z newsami jako data scientist. Zespół data engineering zgromadził bardzo dużą ilość logów o ludziach klikających w artykuły i linki (clickstream data). Użytkownicy nie muszą logować się do systemu, więc identyfikacja jest oparta o ciasteczka (cookies) i niepewna pomiędzy sesjami. Posiadasz za to bogate metadane o użytkowniku (np. rodzaj urządzenia, przeglądarki, geolokacja, historia sesji) oraz o artykułach (np. język, treść, tagi). Jakiego rodzaju systemu rekomendacyjnego byś użył (jakiego da się użyć?) i dlaczego? Nie musisz tutaj opisywać szczegółowo algorytmów. Rozważ:\n",
        "  - globalny vs personalizowany\n",
        "  - content-based vs collaborative filtering vs hybrydowy\n",
        "  - explicit vs implicit\n",
        "\n",
        "4. Pracujesz w firmie tworzącej oprogramowanie dla sklepów internetowych. Użytkownik ma możliwość sortowania malejąco po średniej opinii, ale z historii użyć wynika, że często nie klikają przedmiotów o najwyższej średniej ocenie, tylko któryś z kolejnych. Jaki może być tego przyczyna? Jak można by rozwiązać ten problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NHjTZB6iKQK"
      },
      "source": [
        "// skomentuj tutaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSnaYzHiKQK"
      },
      "source": [
        "Źródła inspiracji do zadań dla zainteresowanych:\n",
        "1. Praca autora tego zestawu.\n",
        "2. Steam API oraz SteamSpy pozwalają łatwo zbierać duże zbiory danych z tej platformy, powstało w ten sposób zresztą całkiem sporo projektów, prac dyplomowych i artykułów naukowych (np. [projekt 1](http://brandonlin.com/steam.pdf), [projekt 2](https://library.ucsd.edu/dc/object/bb5021836n/_3_1.pdf), [praca dyplomowa](https://openaccess.mef.edu.tr/bitstream/handle/20.500.11779/1721/Serhan%20Bayram.pdf?sequence=1&isAllowed=y), [artykuł naukowy 1](https://www.researchgate.net/publication/333072035_Recommender_Systems_for_Online_Video_Game_Platforms_the_Case_of_STEAM), [artykuł naukowy 2](https://trepo.tuni.fi/bitstream/handle/10024/122499/a_hybrid_recommender_system_2020.pdf;jsessionid=E796B8E915FBBF37EF1E0B75210D8690?sequence=2)). Przykładowe zbiory: [dataset 1](https://www.kaggle.com/datasets/nikdavis/steam-store-games), [dataset 2](https://www.kaggle.com/datasets/forgemaster/steam-reviews-dataset), [dataset 3](https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data).\n",
        "3. Luźno wzorowane na [zbiorze danych CI&T Deskdrop](https://www.kaggle.com/datasets/gspmoreira/articles-sharing-reading-from-cit-deskdrop).\n",
        "4. Artykuł [\"How not to sort by average rating\"](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Dep-EaiKQK"
      },
      "source": [
        "## Zadanie dla chętnych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSdJP7UYiKQK"
      },
      "source": [
        "Uruchom na zbiorze MovieLens-1M (albo innym podobnego rozmiaru) algorytm LightGCN ([artykuł](https://arxiv.org/pdf/2002.02126.pdf)), implementujący podejście grafowe do rekomendacji, z użyciem biblioteki LibRecommender ([tutorial](https://github.com/massquantity/LibRecommender/blob/master/examples/pure_ranking_example.py)), która pod spodem używa PyTorch Geometric ([tutorial dla odważnych](https://colab.research.google.com/drive/1VfP6JlWbX_AJnx88yN1tM3BYE6XAADiy?usp=sharing)). Poniżej opis, jak to działa, ale znajomość teorii nie jest potrzebna do wykonania tego zadania :) Możesz użyć domyślnych hiperparametrów architektury sieci z tutoriala, ale zaimplementuj tuning przynajmniej liczby epok (LibRecommender nie ma early stoppingu). Jeżeli zbiór 1M jest za duży dla twojego sprzętu, możesz pracować na 100k. Jeżeli użyjesz MovieLens-1M, dla porównania zaimplementuj także wybrane 1-2 algorytmy z tego laboratorium na tym zbiorze (możesz po prostu skopiować kod z notebooka powyżej).\n",
        "\n",
        "W tym podejściu reprezentujemy problem jako graf, a nie jako macierz. Mamy graf dwudzielny użytkowników i przedmiotów, gdzie ocena reprezentowana jest jako krawędź między wierzchołkiem użytkownika a wierzchołkiem przedmiotu, opisana oceną. Rekomendacja polega na zadaniu **przewidywania krawędzi (edge prediction)**, czyli zasugerowanie dodania nowej krawędzi między użytkownikiem a przedmiotem.\n",
        "\n",
        "Sieć LightGCN implementuje podejście collaborative filtering na grafie. Jest to **grafowa sieć neuronowa (Graph Neural Network, GNN)**, osiągająca obecnie jedne z najlepszych wyników wśród systemów CF. Każdy wierzchołek ma tutaj wektor o pewnej założonej z góry długości $N$, tzw. embedding. Tworzy się go następująco:\n",
        "- robimy one-hot encoding dla użytkowników i przedmiotów, kodując ich `user_id` i `item_id`\n",
        "- mnożymy użytkowników przez macierz, robiąc kombinację liniową i rzutując na niższy wymiar\n",
        "- to samo, co wyżej, tylko dla przedmiotów\n",
        "Macierze embeddujące dla użytkowników i przedmiotów są parametrami, których uczymy się wraz z treningiem sieci neuronowej. Inicjalizuje się je losowo.\n",
        "\n",
        "Sieć LightGCN składa się z kilku warstw **konwolucji grafowej (graph convolution)**, gdzie każda warstwa agreguje informację z sąsiednich wierzchołków. Dla każdego wierzchołka robimy po prostu sumę ważoną wektorów sąsiadów ($e_u$ - embedding użytkownika $u$, $e_i$ - embedding przedmiotu $i$):\n",
        "$$\\large\n",
        "e_u^{(k+1)} = \\sum_{i \\in N(u)} \\frac{1}{\\sqrt{N(u)}\\sqrt{N(i)}} e_i^{(k)}\n",
        "$$\n",
        "Sąsiadów ważymy ich stopniem, aby wziąć pod uwagę popularność poszczególnych przedmiotów i aktywność użytkowników (mają duży stopień). Taka wymiana informacji między wierzchołkami propaguje informację w grafie, aktualizując embeddingi.\n",
        "\n",
        "Typowo takich warstw jest kilka, np. 3-4. Później agreguje się informację ze wszystkich warstw, w odróżnieniu od sieci CNN dla obrazów, gdzie zwykle bierze się wyjście tylko z ostatniej warstwy. Dla każdego użytkownika (i przedmiotu) bierzemy jego embedding z każdej warstwy i uśredniamy je. Daje to bogatą reprezentację wierzchołka i agreguje informacje zarówno z bliskiego sąsiedztwa (głębokie warstwy), jak i z ogółu społecznści w grafie (wysokie warstwy).\n",
        "\n",
        "Predykcja to po prostu iloczyn skalarny embeddingu użytkownika i przedmiotu: $r_{ij} = e_j^T e_i$. Sieć taką uczy się zwykle funkcją kosztu **Bayesian Personalized Ranking (BPR)**, używaną powszechnie w sieciach neuronowych do systemów rekomendacyjnych. Oczywiście uwzględnia się tu wszystkie typowe elementy sieci neuronowych: learning rate, weight decay etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mjyioj2iKQK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}